[{"categories":["实验"],"content":"OVS安装 ","date":"2022-08-28","objectID":"/posts/05_ovs_install/:1:0","series":null,"tags":["ovs"],"title":"mininet , ovs , ryu 分别进行源码安装","uri":"/posts/05_ovs_install/#ovs安装"},{"categories":["实验"],"content":"ovs与linux内核版本对应 OVS版本是需要与linux内核版本对应的，否则会导致内核函数与OVS调用的函数不匹配，简称装不上。 https://docs.openvswitch.org/en/latest/faq/releases/ linux 内核版本查看uname -r ","date":"2022-08-28","objectID":"/posts/05_ovs_install/:1:1","series":null,"tags":["ovs"],"title":"mininet , ovs , ryu 分别进行源码安装","uri":"/posts/05_ovs_install/#ovs与linux内核版本对应"},{"categories":["实验"],"content":"安装过程 这里要注意，如果下载的是官网的压缩文件，则不用./boot.sh直接从./configure开始即可， 如果是在github上下载的，则需要./boot.sh再configure 官方教程: https://docs.openvswitch.org/en/latest/intro/install/# 下载链接: http://www.openvswitch.org/download/ 下载版本: 2.13.6, 解压后进入根目录 参考资料: https://www.cnblogs.com/goldsunshine/p/10331606.html https://blog.csdn.net/zhangmeimei_pku/article/details/97929098 在这之前要先安装python和python-pip, 这么做会让系统自动安装c编译器 sudo apt install python sudo apt install python-pip OVS的源码安装命令如下： ./configure make make install ## 安装内核 make modules_install ## 将openvswitch载入到内核模块中 /sbin/modprobe openvswitch ## 看内核模块是否被加载 /sbin/lsmod |grep openvswitch ## 启动(这里要注意一下使用sudo找不到命令的问题,所以需要找到原来的目录运行一下) export PATH=$PATH:/usr/local/share/openvswitch/scripts ovs-ctl start export PATH=$PATH:/usr/local/share/openvswitch/scripts ovs-ctl --no-ovs-vswitchd start export PATH=$PATH:/usr/local/share/openvswitch/scripts ovs-ctl --no--ovsdb-server start ## 下面的是要配置ovsdb的数据库 (配置数据库了套接字都需要使用sudo,命令找不到就使用which, 然后使用绝对路径运行命令) ## 原文:In addition to using the automated script to start Open vSwitch, you may wish to manually start the various daemons. Before starting ovs-vswitchd itself, you need to start its configuration database, ovsdb-server. Each machine on which Open vSwitch is installed should run its own copy of ovsdb-server. Before ovsdb-server itself can be started, configure a database that it can use: ## 这里要注意, 需要在openvswitch要目录运行该命令 mkdir -p /usr/local/etc/openvswitch ovsdb-tool create /usr/local/etc/openvswitch/conf.db \\ vswitchd/vswitch.ovsschema ## 配置ovsdb-server以使用上面创建的数据库，监听Unix域套接字 ## 原文: Configure ovsdb-server to use database created above, to listen on a Unix domain socket, to connect to any managers specified in the database itself, and to use the SSL configuration in the database: mkdir -p /usr/local/var/run/openvswitch ovsdb-server --remote=punix:/usr/local/var/run/openvswitch/db.sock \\ --remote=db:Open_vSwitch,Open_vSwitch,manager_options \\ --private-key=db:Open_vSwitch,SSL,private_key \\ --certificate=db:Open_vSwitch,SSL,certificate \\ --bootstrap-ca-cert=db:Open_vSwitch,SSL,ca_cert \\ --pidfile --detach --log-file ovs-vsctl --no-wait init ovs-vswitchd --pidfile --detach --log-file 若对OVS的源码有修改，改完ovs之后，再次按照这个步骤重新安装一下自己的ovs 重启系统后: 这个时候启动ovs会失败, 这是因为OVS自带的数据库不会自动启动 因此我们需要把后三个命令再运行一下才行, 即: ovsdb-server --remote=punix:/usr/local/var/run/openvswitch/db.sock \\ --remote=db:Open_vSwitch,Open_vSwitch,manager_options \\ --private-key=db:Open_vSwitch,SSL,private_key \\ --certificate=db:Open_vSwitch,SSL,certificate \\ --bootstrap-ca-cert=db:Open_vSwitch,SSL,ca_cert \\ --pidfile --detach --log-file ovs-vsctl --no-wait init ovs-vswitchd --pidfile --detach --log-file ","date":"2022-08-28","objectID":"/posts/05_ovs_install/:1:2","series":null,"tags":["ovs"],"title":"mininet , ovs , ryu 分别进行源码安装","uri":"/posts/05_ovs_install/#安装过程"},{"categories":["实验"],"content":"自动脚本 这里是我修改OVS源码时为了方便而写的一些自动脚本 OVS删除脚本 修改完OVS源码需要重新安装前需要先停止并删除本机运行的OVS，这是自动脚本： #!/bin/bash # 停止其运行 sudo /usr/local/share/openvswitch/scripts/ovs-ctl stop sleep 2 # 依次删除脚本；还有两个不知道干嘛的 sudo rm -rf /usr/local/share/openvswitch/scripts sudo rm -rf /usr/local/etc/openvswitch/ sudo rm -rf /usr/local/var/run/openvswitch/ sleep 2 # 移除内核模块 echo 'lsmod | grep openvswitch' lsmod | grep openvswitch echo 'sudo rmmod openvswitch' sudo rmmod openvswitch echo 'lsmod | grep openvswitch' lsmod | grep openvswitch # 若移除内核失败,执行下面两条命令 # sudo ovs-dpctl show # 查看是否被占用 # sudo ovs-dpctl del-dp ovs-system # 删除 OVS一键安装脚本 修改完OVS时需要重新安装它，这是它的安装一键脚本，在这之前别忘记删除本机上已安装的OVS,否则易失败 #!/bin/bash ./configure --with-linux=/lib/modules/$(uname -r)/build echo '' echo -e \"\\033[47;30m end configure \\033[0m\" sleep 3 make echo -e \"\\033[47;30m end make \\033[0m\" sleep 3 sudo make install echo -e \"\\033[47;30m end make install \\033[0m\" sleep 3 sudo make modules_install echo -e \"\\033[47;30m end sudo make modules_install \\033[0m\" sleep 3 sudo /sbin/modprobe openvswitch echo -e \"\\033[47;30m end sudo /sbin/modprobe openvswitch \\033[0m\" sleep 3 /sbin/lsmod | grep openvswitch echo -e \"\\033[47;30m end /sbin/lsmod | grep openvswitch \\033[0m\" sleep 3 sudo mkdir -p /usr/local/etc/openvswitch echo -e \"\\033[47;30m end sudo mkdir -p /usr/local/etc/openvswitch \\033[0m\" sleep 3 sudo ovsdb-tool create /usr/local/etc/openvswitch/conf.db vswitchd/vswitch.ovsschema # 这里没出现找不到路径的问题,可能是安装了内核版本的原因 echo -e \"\\033[47;30m end ovsdb-tool \\033[0m\" sleep 3 sudo mkdir -p /usr/local/var/run/openvswitch echo -e \"\\033[47;30m end sudo mkdir -p /usr/local/var/run/openvswitch \\033[0m\" sleep 3 sudo ovsdb-server --remote=punix:/usr/local/var/run/openvswitch/db.sock \\ --remote=db:Open_vSwitch,Open_vSwitch,manager_options \\ --private-key=db:Open_vSwitch,SSL,private_key \\ --certificate=db:Open_vSwitch,SSL,certificate \\ --bootstrap-ca-cert=db:Open_vSwitch,SSL,ca_cert \\ --pidfile --detach --log-file echo -e \"\\033[47;30m end sudo ovsdb-server \\033[0m\" sleep 3 sudo ovs-vsctl --no-wait init echo -e \"\\033[47;30m end sudo ovs-vsctl --no-wait init \\033[0m\" sleep 3 sudo ovs-vswitchd --pidfile --detach --log-file echo -e \"\\033[47;30m end sudo ovs-vswitchd --pidfile --detach --log-file \\033[0m\" sleep 3 sudo mn --test pingall echo -e \"\\033[47;30m end sudo mn --test pingall \\033[0m\" sleep 3 echo -e \"\\033[47;30m show dmesg \\033[0m\" dmesg | tail # 这时能查到我们插入源码的信息 OVS重启脚本 在重启系统后，OVS自带的一些服务数据库啥的不会一同启动，因此需要手动启动 sudo ovsdb-server --remote=punix:/usr/local/var/run/openvswitch/db.sock \\ --remote=db:Open_vSwitch,Open_vSwitch,manager_options \\ --private-key=db:Open_vSwitch,SSL,private_key \\ --certificate=db:Open_vSwitch,SSL,certificate \\ --bootstrap-ca-cert=db:Open_vSwitch,SSL,ca_cert \\ --pidfile --detach --log-file echo -e \"\\033[47;30m sudo ovsdb-server \\033[0m\" sleep 3 sudo ovs-vsctl --no-wait init echo -e \"\\033[47;30m sudo ovs-vsctl --no-wait init \\033[0m\" sleep 3 sudo ovs-vswitchd --pidfile --detach --log-file echo -e \"\\033[47;30m sudo ovs-vswitchd --pidfile --detach --log-file \\033[0m\" OVS迁移脚本 源码被一个系统编译过,再复制到其他系统中编译不成功,需要运行下这个脚本才可 #!/bin/bash autoscan echo -e \"\\033[47;30m autoscan \\033[0m\" sleep 1 aclocal echo -e \"\\033[47;30m aclocal \\033[0m\" sleep 1 autoconf echo -e \"\\033[47;30m autoconf \\033[0m\" sleep 1 automake --add-missing echo -e \"\\033[47;30m automake --add-missing \\033[0m\" ","date":"2022-08-28","objectID":"/posts/05_ovs_install/:1:3","series":null,"tags":["ovs"],"title":"mininet , ovs , ryu 分别进行源码安装","uri":"/posts/05_ovs_install/#自动脚本"},{"categories":["实验"],"content":"自动脚本 这里是我修改OVS源码时为了方便而写的一些自动脚本 OVS删除脚本 修改完OVS源码需要重新安装前需要先停止并删除本机运行的OVS，这是自动脚本： #!/bin/bash # 停止其运行 sudo /usr/local/share/openvswitch/scripts/ovs-ctl stop sleep 2 # 依次删除脚本；还有两个不知道干嘛的 sudo rm -rf /usr/local/share/openvswitch/scripts sudo rm -rf /usr/local/etc/openvswitch/ sudo rm -rf /usr/local/var/run/openvswitch/ sleep 2 # 移除内核模块 echo 'lsmod | grep openvswitch' lsmod | grep openvswitch echo 'sudo rmmod openvswitch' sudo rmmod openvswitch echo 'lsmod | grep openvswitch' lsmod | grep openvswitch # 若移除内核失败,执行下面两条命令 # sudo ovs-dpctl show # 查看是否被占用 # sudo ovs-dpctl del-dp ovs-system # 删除 OVS一键安装脚本 修改完OVS时需要重新安装它，这是它的安装一键脚本，在这之前别忘记删除本机上已安装的OVS,否则易失败 #!/bin/bash ./configure --with-linux=/lib/modules/$(uname -r)/build echo '' echo -e \"\\033[47;30m end configure \\033[0m\" sleep 3 make echo -e \"\\033[47;30m end make \\033[0m\" sleep 3 sudo make install echo -e \"\\033[47;30m end make install \\033[0m\" sleep 3 sudo make modules_install echo -e \"\\033[47;30m end sudo make modules_install \\033[0m\" sleep 3 sudo /sbin/modprobe openvswitch echo -e \"\\033[47;30m end sudo /sbin/modprobe openvswitch \\033[0m\" sleep 3 /sbin/lsmod | grep openvswitch echo -e \"\\033[47;30m end /sbin/lsmod | grep openvswitch \\033[0m\" sleep 3 sudo mkdir -p /usr/local/etc/openvswitch echo -e \"\\033[47;30m end sudo mkdir -p /usr/local/etc/openvswitch \\033[0m\" sleep 3 sudo ovsdb-tool create /usr/local/etc/openvswitch/conf.db vswitchd/vswitch.ovsschema # 这里没出现找不到路径的问题,可能是安装了内核版本的原因 echo -e \"\\033[47;30m end ovsdb-tool \\033[0m\" sleep 3 sudo mkdir -p /usr/local/var/run/openvswitch echo -e \"\\033[47;30m end sudo mkdir -p /usr/local/var/run/openvswitch \\033[0m\" sleep 3 sudo ovsdb-server --remote=punix:/usr/local/var/run/openvswitch/db.sock \\ --remote=db:Open_vSwitch,Open_vSwitch,manager_options \\ --private-key=db:Open_vSwitch,SSL,private_key \\ --certificate=db:Open_vSwitch,SSL,certificate \\ --bootstrap-ca-cert=db:Open_vSwitch,SSL,ca_cert \\ --pidfile --detach --log-file echo -e \"\\033[47;30m end sudo ovsdb-server \\033[0m\" sleep 3 sudo ovs-vsctl --no-wait init echo -e \"\\033[47;30m end sudo ovs-vsctl --no-wait init \\033[0m\" sleep 3 sudo ovs-vswitchd --pidfile --detach --log-file echo -e \"\\033[47;30m end sudo ovs-vswitchd --pidfile --detach --log-file \\033[0m\" sleep 3 sudo mn --test pingall echo -e \"\\033[47;30m end sudo mn --test pingall \\033[0m\" sleep 3 echo -e \"\\033[47;30m show dmesg \\033[0m\" dmesg | tail # 这时能查到我们插入源码的信息 OVS重启脚本 在重启系统后，OVS自带的一些服务数据库啥的不会一同启动，因此需要手动启动 sudo ovsdb-server --remote=punix:/usr/local/var/run/openvswitch/db.sock \\ --remote=db:Open_vSwitch,Open_vSwitch,manager_options \\ --private-key=db:Open_vSwitch,SSL,private_key \\ --certificate=db:Open_vSwitch,SSL,certificate \\ --bootstrap-ca-cert=db:Open_vSwitch,SSL,ca_cert \\ --pidfile --detach --log-file echo -e \"\\033[47;30m sudo ovsdb-server \\033[0m\" sleep 3 sudo ovs-vsctl --no-wait init echo -e \"\\033[47;30m sudo ovs-vsctl --no-wait init \\033[0m\" sleep 3 sudo ovs-vswitchd --pidfile --detach --log-file echo -e \"\\033[47;30m sudo ovs-vswitchd --pidfile --detach --log-file \\033[0m\" OVS迁移脚本 源码被一个系统编译过,再复制到其他系统中编译不成功,需要运行下这个脚本才可 #!/bin/bash autoscan echo -e \"\\033[47;30m autoscan \\033[0m\" sleep 1 aclocal echo -e \"\\033[47;30m aclocal \\033[0m\" sleep 1 autoconf echo -e \"\\033[47;30m autoconf \\033[0m\" sleep 1 automake --add-missing echo -e \"\\033[47;30m automake --add-missing \\033[0m\" ","date":"2022-08-28","objectID":"/posts/05_ovs_install/:1:3","series":null,"tags":["ovs"],"title":"mininet , ovs , ryu 分别进行源码安装","uri":"/posts/05_ovs_install/#ovs删除脚本"},{"categories":["实验"],"content":"自动脚本 这里是我修改OVS源码时为了方便而写的一些自动脚本 OVS删除脚本 修改完OVS源码需要重新安装前需要先停止并删除本机运行的OVS，这是自动脚本： #!/bin/bash # 停止其运行 sudo /usr/local/share/openvswitch/scripts/ovs-ctl stop sleep 2 # 依次删除脚本；还有两个不知道干嘛的 sudo rm -rf /usr/local/share/openvswitch/scripts sudo rm -rf /usr/local/etc/openvswitch/ sudo rm -rf /usr/local/var/run/openvswitch/ sleep 2 # 移除内核模块 echo 'lsmod | grep openvswitch' lsmod | grep openvswitch echo 'sudo rmmod openvswitch' sudo rmmod openvswitch echo 'lsmod | grep openvswitch' lsmod | grep openvswitch # 若移除内核失败,执行下面两条命令 # sudo ovs-dpctl show # 查看是否被占用 # sudo ovs-dpctl del-dp ovs-system # 删除 OVS一键安装脚本 修改完OVS时需要重新安装它，这是它的安装一键脚本，在这之前别忘记删除本机上已安装的OVS,否则易失败 #!/bin/bash ./configure --with-linux=/lib/modules/$(uname -r)/build echo '' echo -e \"\\033[47;30m end configure \\033[0m\" sleep 3 make echo -e \"\\033[47;30m end make \\033[0m\" sleep 3 sudo make install echo -e \"\\033[47;30m end make install \\033[0m\" sleep 3 sudo make modules_install echo -e \"\\033[47;30m end sudo make modules_install \\033[0m\" sleep 3 sudo /sbin/modprobe openvswitch echo -e \"\\033[47;30m end sudo /sbin/modprobe openvswitch \\033[0m\" sleep 3 /sbin/lsmod | grep openvswitch echo -e \"\\033[47;30m end /sbin/lsmod | grep openvswitch \\033[0m\" sleep 3 sudo mkdir -p /usr/local/etc/openvswitch echo -e \"\\033[47;30m end sudo mkdir -p /usr/local/etc/openvswitch \\033[0m\" sleep 3 sudo ovsdb-tool create /usr/local/etc/openvswitch/conf.db vswitchd/vswitch.ovsschema # 这里没出现找不到路径的问题,可能是安装了内核版本的原因 echo -e \"\\033[47;30m end ovsdb-tool \\033[0m\" sleep 3 sudo mkdir -p /usr/local/var/run/openvswitch echo -e \"\\033[47;30m end sudo mkdir -p /usr/local/var/run/openvswitch \\033[0m\" sleep 3 sudo ovsdb-server --remote=punix:/usr/local/var/run/openvswitch/db.sock \\ --remote=db:Open_vSwitch,Open_vSwitch,manager_options \\ --private-key=db:Open_vSwitch,SSL,private_key \\ --certificate=db:Open_vSwitch,SSL,certificate \\ --bootstrap-ca-cert=db:Open_vSwitch,SSL,ca_cert \\ --pidfile --detach --log-file echo -e \"\\033[47;30m end sudo ovsdb-server \\033[0m\" sleep 3 sudo ovs-vsctl --no-wait init echo -e \"\\033[47;30m end sudo ovs-vsctl --no-wait init \\033[0m\" sleep 3 sudo ovs-vswitchd --pidfile --detach --log-file echo -e \"\\033[47;30m end sudo ovs-vswitchd --pidfile --detach --log-file \\033[0m\" sleep 3 sudo mn --test pingall echo -e \"\\033[47;30m end sudo mn --test pingall \\033[0m\" sleep 3 echo -e \"\\033[47;30m show dmesg \\033[0m\" dmesg | tail # 这时能查到我们插入源码的信息 OVS重启脚本 在重启系统后，OVS自带的一些服务数据库啥的不会一同启动，因此需要手动启动 sudo ovsdb-server --remote=punix:/usr/local/var/run/openvswitch/db.sock \\ --remote=db:Open_vSwitch,Open_vSwitch,manager_options \\ --private-key=db:Open_vSwitch,SSL,private_key \\ --certificate=db:Open_vSwitch,SSL,certificate \\ --bootstrap-ca-cert=db:Open_vSwitch,SSL,ca_cert \\ --pidfile --detach --log-file echo -e \"\\033[47;30m sudo ovsdb-server \\033[0m\" sleep 3 sudo ovs-vsctl --no-wait init echo -e \"\\033[47;30m sudo ovs-vsctl --no-wait init \\033[0m\" sleep 3 sudo ovs-vswitchd --pidfile --detach --log-file echo -e \"\\033[47;30m sudo ovs-vswitchd --pidfile --detach --log-file \\033[0m\" OVS迁移脚本 源码被一个系统编译过,再复制到其他系统中编译不成功,需要运行下这个脚本才可 #!/bin/bash autoscan echo -e \"\\033[47;30m autoscan \\033[0m\" sleep 1 aclocal echo -e \"\\033[47;30m aclocal \\033[0m\" sleep 1 autoconf echo -e \"\\033[47;30m autoconf \\033[0m\" sleep 1 automake --add-missing echo -e \"\\033[47;30m automake --add-missing \\033[0m\" ","date":"2022-08-28","objectID":"/posts/05_ovs_install/:1:3","series":null,"tags":["ovs"],"title":"mininet , ovs , ryu 分别进行源码安装","uri":"/posts/05_ovs_install/#ovs一键安装脚本"},{"categories":["实验"],"content":"自动脚本 这里是我修改OVS源码时为了方便而写的一些自动脚本 OVS删除脚本 修改完OVS源码需要重新安装前需要先停止并删除本机运行的OVS，这是自动脚本： #!/bin/bash # 停止其运行 sudo /usr/local/share/openvswitch/scripts/ovs-ctl stop sleep 2 # 依次删除脚本；还有两个不知道干嘛的 sudo rm -rf /usr/local/share/openvswitch/scripts sudo rm -rf /usr/local/etc/openvswitch/ sudo rm -rf /usr/local/var/run/openvswitch/ sleep 2 # 移除内核模块 echo 'lsmod | grep openvswitch' lsmod | grep openvswitch echo 'sudo rmmod openvswitch' sudo rmmod openvswitch echo 'lsmod | grep openvswitch' lsmod | grep openvswitch # 若移除内核失败,执行下面两条命令 # sudo ovs-dpctl show # 查看是否被占用 # sudo ovs-dpctl del-dp ovs-system # 删除 OVS一键安装脚本 修改完OVS时需要重新安装它，这是它的安装一键脚本，在这之前别忘记删除本机上已安装的OVS,否则易失败 #!/bin/bash ./configure --with-linux=/lib/modules/$(uname -r)/build echo '' echo -e \"\\033[47;30m end configure \\033[0m\" sleep 3 make echo -e \"\\033[47;30m end make \\033[0m\" sleep 3 sudo make install echo -e \"\\033[47;30m end make install \\033[0m\" sleep 3 sudo make modules_install echo -e \"\\033[47;30m end sudo make modules_install \\033[0m\" sleep 3 sudo /sbin/modprobe openvswitch echo -e \"\\033[47;30m end sudo /sbin/modprobe openvswitch \\033[0m\" sleep 3 /sbin/lsmod | grep openvswitch echo -e \"\\033[47;30m end /sbin/lsmod | grep openvswitch \\033[0m\" sleep 3 sudo mkdir -p /usr/local/etc/openvswitch echo -e \"\\033[47;30m end sudo mkdir -p /usr/local/etc/openvswitch \\033[0m\" sleep 3 sudo ovsdb-tool create /usr/local/etc/openvswitch/conf.db vswitchd/vswitch.ovsschema # 这里没出现找不到路径的问题,可能是安装了内核版本的原因 echo -e \"\\033[47;30m end ovsdb-tool \\033[0m\" sleep 3 sudo mkdir -p /usr/local/var/run/openvswitch echo -e \"\\033[47;30m end sudo mkdir -p /usr/local/var/run/openvswitch \\033[0m\" sleep 3 sudo ovsdb-server --remote=punix:/usr/local/var/run/openvswitch/db.sock \\ --remote=db:Open_vSwitch,Open_vSwitch,manager_options \\ --private-key=db:Open_vSwitch,SSL,private_key \\ --certificate=db:Open_vSwitch,SSL,certificate \\ --bootstrap-ca-cert=db:Open_vSwitch,SSL,ca_cert \\ --pidfile --detach --log-file echo -e \"\\033[47;30m end sudo ovsdb-server \\033[0m\" sleep 3 sudo ovs-vsctl --no-wait init echo -e \"\\033[47;30m end sudo ovs-vsctl --no-wait init \\033[0m\" sleep 3 sudo ovs-vswitchd --pidfile --detach --log-file echo -e \"\\033[47;30m end sudo ovs-vswitchd --pidfile --detach --log-file \\033[0m\" sleep 3 sudo mn --test pingall echo -e \"\\033[47;30m end sudo mn --test pingall \\033[0m\" sleep 3 echo -e \"\\033[47;30m show dmesg \\033[0m\" dmesg | tail # 这时能查到我们插入源码的信息 OVS重启脚本 在重启系统后，OVS自带的一些服务数据库啥的不会一同启动，因此需要手动启动 sudo ovsdb-server --remote=punix:/usr/local/var/run/openvswitch/db.sock \\ --remote=db:Open_vSwitch,Open_vSwitch,manager_options \\ --private-key=db:Open_vSwitch,SSL,private_key \\ --certificate=db:Open_vSwitch,SSL,certificate \\ --bootstrap-ca-cert=db:Open_vSwitch,SSL,ca_cert \\ --pidfile --detach --log-file echo -e \"\\033[47;30m sudo ovsdb-server \\033[0m\" sleep 3 sudo ovs-vsctl --no-wait init echo -e \"\\033[47;30m sudo ovs-vsctl --no-wait init \\033[0m\" sleep 3 sudo ovs-vswitchd --pidfile --detach --log-file echo -e \"\\033[47;30m sudo ovs-vswitchd --pidfile --detach --log-file \\033[0m\" OVS迁移脚本 源码被一个系统编译过,再复制到其他系统中编译不成功,需要运行下这个脚本才可 #!/bin/bash autoscan echo -e \"\\033[47;30m autoscan \\033[0m\" sleep 1 aclocal echo -e \"\\033[47;30m aclocal \\033[0m\" sleep 1 autoconf echo -e \"\\033[47;30m autoconf \\033[0m\" sleep 1 automake --add-missing echo -e \"\\033[47;30m automake --add-missing \\033[0m\" ","date":"2022-08-28","objectID":"/posts/05_ovs_install/:1:3","series":null,"tags":["ovs"],"title":"mininet , ovs , ryu 分别进行源码安装","uri":"/posts/05_ovs_install/#ovs重启脚本"},{"categories":["实验"],"content":"自动脚本 这里是我修改OVS源码时为了方便而写的一些自动脚本 OVS删除脚本 修改完OVS源码需要重新安装前需要先停止并删除本机运行的OVS，这是自动脚本： #!/bin/bash # 停止其运行 sudo /usr/local/share/openvswitch/scripts/ovs-ctl stop sleep 2 # 依次删除脚本；还有两个不知道干嘛的 sudo rm -rf /usr/local/share/openvswitch/scripts sudo rm -rf /usr/local/etc/openvswitch/ sudo rm -rf /usr/local/var/run/openvswitch/ sleep 2 # 移除内核模块 echo 'lsmod | grep openvswitch' lsmod | grep openvswitch echo 'sudo rmmod openvswitch' sudo rmmod openvswitch echo 'lsmod | grep openvswitch' lsmod | grep openvswitch # 若移除内核失败,执行下面两条命令 # sudo ovs-dpctl show # 查看是否被占用 # sudo ovs-dpctl del-dp ovs-system # 删除 OVS一键安装脚本 修改完OVS时需要重新安装它，这是它的安装一键脚本，在这之前别忘记删除本机上已安装的OVS,否则易失败 #!/bin/bash ./configure --with-linux=/lib/modules/$(uname -r)/build echo '' echo -e \"\\033[47;30m end configure \\033[0m\" sleep 3 make echo -e \"\\033[47;30m end make \\033[0m\" sleep 3 sudo make install echo -e \"\\033[47;30m end make install \\033[0m\" sleep 3 sudo make modules_install echo -e \"\\033[47;30m end sudo make modules_install \\033[0m\" sleep 3 sudo /sbin/modprobe openvswitch echo -e \"\\033[47;30m end sudo /sbin/modprobe openvswitch \\033[0m\" sleep 3 /sbin/lsmod | grep openvswitch echo -e \"\\033[47;30m end /sbin/lsmod | grep openvswitch \\033[0m\" sleep 3 sudo mkdir -p /usr/local/etc/openvswitch echo -e \"\\033[47;30m end sudo mkdir -p /usr/local/etc/openvswitch \\033[0m\" sleep 3 sudo ovsdb-tool create /usr/local/etc/openvswitch/conf.db vswitchd/vswitch.ovsschema # 这里没出现找不到路径的问题,可能是安装了内核版本的原因 echo -e \"\\033[47;30m end ovsdb-tool \\033[0m\" sleep 3 sudo mkdir -p /usr/local/var/run/openvswitch echo -e \"\\033[47;30m end sudo mkdir -p /usr/local/var/run/openvswitch \\033[0m\" sleep 3 sudo ovsdb-server --remote=punix:/usr/local/var/run/openvswitch/db.sock \\ --remote=db:Open_vSwitch,Open_vSwitch,manager_options \\ --private-key=db:Open_vSwitch,SSL,private_key \\ --certificate=db:Open_vSwitch,SSL,certificate \\ --bootstrap-ca-cert=db:Open_vSwitch,SSL,ca_cert \\ --pidfile --detach --log-file echo -e \"\\033[47;30m end sudo ovsdb-server \\033[0m\" sleep 3 sudo ovs-vsctl --no-wait init echo -e \"\\033[47;30m end sudo ovs-vsctl --no-wait init \\033[0m\" sleep 3 sudo ovs-vswitchd --pidfile --detach --log-file echo -e \"\\033[47;30m end sudo ovs-vswitchd --pidfile --detach --log-file \\033[0m\" sleep 3 sudo mn --test pingall echo -e \"\\033[47;30m end sudo mn --test pingall \\033[0m\" sleep 3 echo -e \"\\033[47;30m show dmesg \\033[0m\" dmesg | tail # 这时能查到我们插入源码的信息 OVS重启脚本 在重启系统后，OVS自带的一些服务数据库啥的不会一同启动，因此需要手动启动 sudo ovsdb-server --remote=punix:/usr/local/var/run/openvswitch/db.sock \\ --remote=db:Open_vSwitch,Open_vSwitch,manager_options \\ --private-key=db:Open_vSwitch,SSL,private_key \\ --certificate=db:Open_vSwitch,SSL,certificate \\ --bootstrap-ca-cert=db:Open_vSwitch,SSL,ca_cert \\ --pidfile --detach --log-file echo -e \"\\033[47;30m sudo ovsdb-server \\033[0m\" sleep 3 sudo ovs-vsctl --no-wait init echo -e \"\\033[47;30m sudo ovs-vsctl --no-wait init \\033[0m\" sleep 3 sudo ovs-vswitchd --pidfile --detach --log-file echo -e \"\\033[47;30m sudo ovs-vswitchd --pidfile --detach --log-file \\033[0m\" OVS迁移脚本 源码被一个系统编译过,再复制到其他系统中编译不成功,需要运行下这个脚本才可 #!/bin/bash autoscan echo -e \"\\033[47;30m autoscan \\033[0m\" sleep 1 aclocal echo -e \"\\033[47;30m aclocal \\033[0m\" sleep 1 autoconf echo -e \"\\033[47;30m autoconf \\033[0m\" sleep 1 automake --add-missing echo -e \"\\033[47;30m automake --add-missing \\033[0m\" ","date":"2022-08-28","objectID":"/posts/05_ovs_install/:1:3","series":null,"tags":["ovs"],"title":"mininet , ovs , ryu 分别进行源码安装","uri":"/posts/05_ovs_install/#ovs迁移脚本"},{"categories":["实验"],"content":"Mininet安装(官网上的安装教程最全) 下载:http://mininet.org/download/ 去github上下载它的源码 解压后进入根目录 使用git tag可以查看所有的版本, 不切换当前默认安装3.2.1b1. 使用的安装命令: ## 只安装mininet和openflow1.3, 因此使用命令: ./util/install.sh -n3 ","date":"2022-08-28","objectID":"/posts/05_ovs_install/:2:0","series":null,"tags":["ovs"],"title":"mininet , ovs , ryu 分别进行源码安装","uri":"/posts/05_ovs_install/#mininet安装官网上的安装教程最全"},{"categories":["实验"],"content":"测试 运行 sudo mn –test pingall, 成功则说明2个都安装好了 *** Creating network *** Adding controller *** Adding hosts: h1 h2 *** Adding switches: s1 *** Adding links: (h1, s1) (h2, s1) *** Configuring hosts h1 h2 *** Starting controller c0 *** Starting 1 switches s1 ... *** Waiting for switches to connect s1 *** Ping: testing ping reachability h1 -\u003e h2 h2 -\u003e h1 *** Results: 0% dropped (2/2 received) *** Stopping 1 controllers c0 *** Stopping 2 links .. *** Stopping 1 switches s1 *** Stopping 2 hosts h1 h2 *** Done completed in 6.082 seconds ","date":"2022-08-28","objectID":"/posts/05_ovs_install/:3:0","series":null,"tags":["ovs"],"title":"mininet , ovs , ryu 分别进行源码安装","uri":"/posts/05_ovs_install/#测试"},{"categories":["实验"],"content":"ryu源码安装 下载源码 git clone https://github.com/faucetsdn/ryu.git github上的readme里面有相关的安装依赖说明, 直接使用pip安装没成功, 因此我们这里使用python3安装. 安装pip3 sudo apt install python3-pip 安装依赖 sudo pip3 install -r tools/pip-requires 安装ryu(此之前应已安装python3) sudo python3 setup.py install 运行 ryu-manager loading app ryu.controller.ofp_handler instantiating app ryu.controller.ofp_handler of OFPHandler 成功 ","date":"2022-08-28","objectID":"/posts/05_ovs_install/:4:0","series":null,"tags":["ovs"],"title":"mininet , ovs , ryu 分别进行源码安装","uri":"/posts/05_ovs_install/#ryu源码安装"},{"categories":null,"content":"摘要 随着智能工厂的空前发展，工业网络中需要更加频繁更改网络流量的转发策略。软件定义网络具有高灵活性和可编程性的优点，可动态地管理网络流量转发策略，它对于智能工业网络具有很可观的发展前景。然而，软件定义工业网络十分容易受到网络攻击，网络攻击会导致工业生产效率降低甚至引起工业事故。本文提出一个基于深度学习的单类入侵检测方案（DO-IDS）来提高工业网络的安全性。首先，DO-IDS 定期抽取工业网络流量的流统计信息并据此生成网络状态特征；其次，我们利用一种基于深度学习的降维方法来过滤上述特征中的冗余特征；此外，我们设计一种基于深度学习的单类检测器来计算网络特征的异常打分；最后，我们对所提出的检测方案进行了模拟实验，结果表明 DO-IDS 可以高效率且高准确率地检测出异常流量。 关键字：工业网络安全；入侵检测；单类深度学习；软件定义工业网络（SDIN） ","date":"2022-08-23","objectID":"/hide/do-ids%E6%B1%89%E8%AF%AD%E7%89%88/:1:0","series":null,"tags":null,"title":"DO-IDS汉语版","uri":"/hide/do-ids%E6%B1%89%E8%AF%AD%E7%89%88/#摘要"},{"categories":null,"content":"介绍 智能工厂的快速发展和革新极大地提高了工厂的生产效率并降低了生产代价。越来越多的工厂模式（如私人定制、远程控制、数据孪生等）需要频繁地自定义或更改网络流量转发策略，这些功能需要一个灵活的工业网络架构才能实现。然而，传统网络的转发功能函数是直接实现在硬件中，这种实现方式使得网络管理员需要手动对网络设备进行配置才可更改流量转发策略。 软件定义网络解偶了网络的数据平面和控制平面，在逻辑上集中式且可编程的控制平面使得SDN具有极大的灵活性。数据平面只需要根据从控制平面处接收到的指令转发网络流量即可，这种简单的功能可以极大地降低数据平面的硬件成本。因此对于智能工厂而言，软件定义工业网络（SDIN）具有很大的发展潜力。 工业网络中工业设备间的安全通信对于构建安全的工业环境，定制生产策略和多生产线的同步都至关重要。然而，软件定义工业网络的这种架构十分容易受到网络攻击。中心化的控制平面面临着单点失效的问题，数据平面与控制平面的通信信道可能会遭受分布式拒绝服务攻击和中间人攻击。此外，传统网络所面临的网络攻击，如 Probing, R2I, U2R, Trojan, Worm 等也均会在软件定义工业网络环境中出现，这些网络攻击会降低工业生产效率，甚至会导致严重的工业事故。例如，美国的燃气公司曾在2021年的5月遭受过一次网络攻击，该攻击导致燃气管道关闭了数日；世界上最大的钢铁生产商也曾遭受过网络攻击并导致了北美区域的钢铁产量下降。 入侵检测技术能够监控网络状态，识别异常网络流量和网络行为，并在网络异常发生时及时警告网络管理员从而降低由网络攻击所造成的损失。然而，在软件定义工业网络环境中的进行入侵检测仍然存在着一些挑战。首先，所有的智能功能都存在于逻辑上集中的控制器中，若要在当前的高带宽网络环境中对每一个数据包进行检查必定会给控制器带来极大的负担，因此入侵检测技术应足够地轻量级以减少控制器的负担。其次，网络攻击虽然无时无刻不在网络中发生，但网络中的正常流量仍然占据了绝大多数，当前网络环境中流量极大，因此手动标识每个数据包是否正常十分地不可行。再次，网络流量的分布模式随网络设备和网络应用的变化而变化，因此当前已存在的检测模型和数据集会逐渐过时，并造成入侵检测的高误报率。最后，控制器所收集的信息中，冗余和无用信息会极大地降低入侵检测模型的检测性能。 为提高工业网络的安全性并克服以上挑战，本文提出了一种基于深度学习的单类入侵检测机制（DO-IDS）来识别软件定义工业网络环境的安全。DO-IDS有三个模块组成：收集器（Collector），检测器（Detector）和防御器（Defender）。收集器负责定期从数据平面抽取网络流的特征信息。检测器负责过滤所抽取特征信息中的冗余信息，再利用基于长短期记忆网络的自编码器（LAE）将特征信息编码为固定长度的向量并计算该向量的异常分数。由 deep-SVDD 启发，我们提出了一种基于深度学习的单类检测算法来计算流量特征的异常分数。防御器会根据检测器所计算出的异常分数来选择合适的流量策略。本文贡献总结如下： 提出了一个基于深度学习的单类异常检测方案，该方案可有效地监控网络状态并检测网络中的异常行为从而提高工业网络安全性。 为克服当前无法获得有标签且最新训练数据集的问题，我们提出一种基于单类深度学习的异常发现算法，并提出针对该算法提出了三个优化：压缩，异常样本利用以及联合训练。 提出利用LAE来将所收集的网络流特征编码为固定长度的向量，从而过滤掉所收集特征信息的冗余信息并方便异常分数的计算。 对所提出的方法进行了模拟实验，实验结果表明相较于当前异常检测算法，本文提出的检测方案在不平衡数据集上能够取得很好的检测效果。 ","date":"2022-08-23","objectID":"/hide/do-ids%E6%B1%89%E8%AF%AD%E7%89%88/:2:0","series":null,"tags":null,"title":"DO-IDS汉语版","uri":"/hide/do-ids%E6%B1%89%E8%AF%AD%E7%89%88/#介绍"},{"categories":null,"content":"相关工作 近年来学者们提出了很多网络入侵和异常检测相关的研究工作，这些工作基本可分为基于熵的，基于机器学习的和基于深度学习的。在本部分我们对前人的针对网络入侵和异常检测相关的工作进行介绍。 ","date":"2022-08-23","objectID":"/hide/do-ids%E6%B1%89%E8%AF%AD%E7%89%88/:3:0","series":null,"tags":null,"title":"DO-IDS汉语版","uri":"/hide/do-ids%E6%B1%89%E8%AF%AD%E7%89%88/#相关工作"},{"categories":null,"content":"基于熵的入侵检测 数据样本的熵值可以用来表示样本分布的随机性，熵值越高表示样本的分布越离散。文献[12]中提出了一种基于熵的轻量级 DDoS 泛洪攻击检测方法，该方法通过利用网络中的边缘交换机来收集网络流的统计信息从而达到检测的目的。文献[13]中提出一种基于联合熵的流量异常检测方法，该方法通过动态选择可疑网络数据包的属性并计算它们的联合熵来提高检测性能。然而，在基于熵的方法中，需要监控的网络数据包的属性需要由网络管理员手动指定，这大大降低了检测方法的易用性。 ","date":"2022-08-23","objectID":"/hide/do-ids%E6%B1%89%E8%AF%AD%E7%89%88/:3:1","series":null,"tags":null,"title":"DO-IDS汉语版","uri":"/hide/do-ids%E6%B1%89%E8%AF%AD%E7%89%88/#基于熵的入侵检测"},{"categories":null,"content":"基于机器学习的入侵检测 机器学习技术十分擅长在超大数据集中找出数据的发展趋势和分布模式，且该过程自动进行不需要人为干预。此外，当我们将网络流量聚焦为流记录时，所需要收集的数据量会大大减少，这使得收集网络流记录并结合机器学习的异常检测方法越来越受欢迎[14]。文献[15]中提出在SDN环境中收集网络流特征信息，并用收集的信息训练有监督分类模型从而监控网络状态。文献[16]中提出使用受限波尔兹曼机将所收集的网络流特征信息转换为固定长度的向量并利用支持向量机算法对其进行异常检测。虽然有监督的分类算法分类正确率高，但收集有标签且当下未过时的网络流量数据样本十分困难。 ","date":"2022-08-23","objectID":"/hide/do-ids%E6%B1%89%E8%AF%AD%E7%89%88/:3:2","series":null,"tags":null,"title":"DO-IDS汉语版","uri":"/hide/do-ids%E6%B1%89%E8%AF%AD%E7%89%88/#基于机器学习的入侵检测"},{"categories":null,"content":"基于深度学习的入侵检测 深度学习技术可以在大数据集中找出数据间的隐藏关系，这个优点使得该技术广受研究人员的偏爱[17][18]。文献[19]利用所收集的网络流统计信息来训练深度神经网络模型从而进行网络入侵检测。文献[20]提出一种将特征筛选和门递归单元长短期记忆网络相结合的方法来监督网络流特征是否发生异常。文献[21]中提出训练自编码器网络来重构流量样本，并对重构误差进行聚类。若一个流量样本的重构误差不属于已知的任何一类，则该样本被视为异常。现有的基于深度学习的检测方法要么需要二类数据进行训练，要么只能使用正常数据进行训练。 本文提出 DO-IDS 来提高软件定义网络环境的安全性。与已知的工作相比，DO-IDS 通过利用 LAE 来将特征序列编码为固定长度的向量，这使得 DO-IDS 有处理网络特征序列的能力，且利用 LAE 获得固定长度向量的过程可降低序列信息的冗余信息从而提高 DO-IDS 的检测正确率。此外，DO-IDS 的训练过程中既可以只使用单类正常数据作为训练样本，也可以使用有标签的数据集，并能够定期更新检测器的检测参数从而持续保证检测性能。 ","date":"2022-08-23","objectID":"/hide/do-ids%E6%B1%89%E8%AF%AD%E7%89%88/:3:3","series":null,"tags":null,"title":"DO-IDS汉语版","uri":"/hide/do-ids%E6%B1%89%E8%AF%AD%E7%89%88/#基于深度学习的入侵检测"},{"categories":["科研学习"],"content":"选择的流经过所有的交换机 控制器实时构造rule graph 获取所有的ingress rule（入度为0的规则），并依长度降序排序 对每一个ingress rule \\(r_0\\): 若该path的节点未在节点集\\(\\mathbb{S}\\)中： 将$r_0$所属的rule path所经过的节点加入$\\mathbb{S}$中； 将 \\(r_0\\) 所属的rule path放到规则集$\\mathbb{R}$中。 若节点集的节点数等于规则图中的节点数（这表明每一个交换机都有经过其的rule），则结束. 只针对交换机的流规则选择办法中，当攻击者对某个流进行恶意交付时，无法被检测。 ","date":"2022-08-23","objectID":"/hide/org_08_rule_selection/:1:0","series":null,"tags":["idea"],"title":"记录在数据平面选择流的过程","uri":"/hide/org_08_rule_selection/#选择的流经过所有的交换机"},{"categories":["科研学习"],"content":"选择的流经过所有的输出端口 控制器实时构造rule graph 针对每个交换机的输出端口，记为集合\\(\\mathbb{O}\\). 计算每个rule path所经过的输出端口数。 每次选择经过$\\mathbb{O}$中端口数最多的rule path \\(r\\),记$O(r)$为规则$r$所经过的输出端口 \\(\\mathbb{O} = \\mathbb{O} / \\{O( r)\\}\\) \\(\\mathbb{R} = \\mathbb{R} + \\{r\\}\\) 若\\(|\\mathbb{O}| == 0\\)，则算法结束，返回\\(\\mathbb{R}\\) ","date":"2022-08-23","objectID":"/hide/org_08_rule_selection/:2:0","series":null,"tags":["idea"],"title":"记录在数据平面选择流的过程","uri":"/hide/org_08_rule_selection/#选择的流经过所有的输出端口"},{"categories":["科研学习"],"content":"文章目的 介绍一个用于在有向无环图（DAG）中进行最小路径覆盖（Minimum Path Cover, MPC）的算法。文中给定了一个场景（交通网络中的运输问题），并将这个场景建模成MPC问题，最后给出了求解方法。 ","date":"2022-08-21","objectID":"/posts/org_06_mpc/:1:0","series":null,"tags":["论文阅读"],"title":"Solving Minimum Path Cover on a DAG","uri":"/posts/org_06_mpc/#文章目的"},{"categories":["科研学习"],"content":"情景样例 假设一个交通运输系统中有4个地铁站，站与站之间可达，交通时间表可确定。你需要做的是购买车票从而实现所有的运输。时间表如下： 最坏的情况是每辆车都买一个票，需要7张。但其实有的运输任务可被同一个车完成，如T1和T2。我们需要做的是最大化运输效率，即最小化所构买的车票数，此时，找到了一个目标函数。 最优情况是一列车走过所有的站，这时只要买一张票即足够，这种情况不存在，因此我们把问题定义为： 最大化车所到达的站数的同时，保证所有的站点均到达。 首先根据传输时间表构造交通图，因为车辆到达时间是不同的且有先后的，所以车辆换乘是有时间依赖的，据此构造图如下: 因此，现在问题转变为: 在以上构造的图上找到MPC。 ","date":"2022-08-21","objectID":"/posts/org_06_mpc/:2:0","series":null,"tags":["论文阅读"],"title":"Solving Minimum Path Cover on a DAG","uri":"/posts/org_06_mpc/#情景样例"},{"categories":["科研学习"],"content":"解决办法 在通用的图上找MPC是一个NP-hard问题，但在DAG中找MPC可转换为找二部图的最大匹配问题，这可以在多项式时间内完成。 为解决以上问题，首先构造一个辅助图，在辅助图中，将原始图中的每个节点分为两个，一个节点保留原始节点的出度，另一个节点保留原始图的入度。举个例子，将节点$t$划分为\\(t_1, t_2\\). 对于t的邻居，从$t_1$画边连出去，对于将$t$作为邻居的其他节点，将该节点画边连到\\(t_2\\)。如此构造的图其实是一个二部图，结果如下: 在以上二部图中查找匹配，假如找完最大匹配后某一个节点未到达，则需要单独为该节点购买一个车次的票以完成运输。因此最小化未到达节点的问题即是上图中的查找最大匹配的问题。而二部图的最大匹配问题是可以在多项式时间内完成的。 其中的一个匹配结果为： ","date":"2022-08-21","objectID":"/posts/org_06_mpc/:3:0","series":null,"tags":["论文阅读"],"title":"Solving Minimum Path Cover on a DAG","uri":"/posts/org_06_mpc/#解决办法"},{"categories":["科研学习"],"content":"二部图最大匹配 对于二部图的匹配问题其是就是用匈牙利算法来解决的。这里简单介绍一下。参考自这里，这个作者还有其他算法的讲解以及代码实现。 对于现有的二部图 要找到它的最大匹配，是需要遍历它所有的节点的: 对于B1, 它与G2和G4都有连接，这里先将它与G2相连。 对于B2，它只与G2相连，这时再返回去看与G2相连的B1是否有别的相连的，可见G4可选，因此就将G2与B2相连，B1与G4相连； 对于B3，它与G1和G3相连，G1和G3都均未与其他节点相连，可将它连上G1 对于B4, 它只能与G4相连，但G4已与B1相连。B1除了G4，还可以与G2相连，但G2已与B2相连了，因此如果B4与G4相连了，B1就无法与别的节点相加，所以B4就无法匹配了。 至此，最大匹配结束。上述过程可见计算的复杂度是二部图两边节点之积。 ","date":"2022-08-21","objectID":"/posts/org_06_mpc/:4:0","series":null,"tags":["论文阅读"],"title":"Solving Minimum Path Cover on a DAG","uri":"/posts/org_06_mpc/#二部图最大匹配"},{"categories":["额外学习"],"content":"Magit 的基本介绍 Magit是在emacs中对git的一种扩展, 它的本质是在emacs中, 利用emacs的方式调用git的命令, 使得git的使用更加地emacs化. 本人的emacs 能力有限, 因此只介绍magit中的一些最常用的命令, 方便日常使用够用即可. 要更加全面地研究可以看它的原网站. ","date":"2022-08-15","objectID":"/posts/org_05_magit_usage/:1:0","series":null,"tags":["git"],"title":"Magit的基本使用","uri":"/posts/org_05_magit_usage/#magit-的基本介绍"},{"categories":["额外学习"],"content":"Magit的基本介绍 ","date":"2022-08-15","objectID":"/posts/org_05_magit_usage/:2:0","series":null,"tags":["git"],"title":"Magit的基本使用","uri":"/posts/org_05_magit_usage/#magit的基本介绍"},{"categories":["额外学习"],"content":"打开Magit窗口: 使用Emacs打开任一由git管理的文件, 再使用快捷键 C-x g (M-x magit-status) 即可打开. 基本上第一次需要使用M-x magit-status打开, 后面才能用C-x g快捷键, 不知道是不是什么bug. 打开后会有一个窗口(这里我事先建好了一个 git 仓库) 分析下magit 窗口每部分的意思: Head: 当前HEAD指向 Untracked files: 未追踪的文件 (也就是没使用 git add 命令添加的文件) Unstaged changes: 由git管理, 进行了修改, 但未添加到暂存区的文件 Recent commits: 最近的提交记录, 第一列就是提交ID 如果常使用git, 以上解释应该一遍就能记住 ","date":"2022-08-15","objectID":"/posts/org_05_magit_usage/:2:1","series":null,"tags":["git"],"title":"Magit的基本使用","uri":"/posts/org_05_magit_usage/#打开magit窗口"},{"categories":["额外学习"],"content":"常用的一些快捷键 g \u0026 G: g可以刷新当前buffer, G刷新全部buffer. 什么意思呢, 比如当前你在查看magit面板, 但是突发奇想往工程里添加了个文件, 这时magit面板的Untracked files里应该要多一个刚添加的文件的, 但是并没有, 此时需要按一下g, 这时这个新加的文件就会出现然Untracked files里了. TAB: 折叠, 即如果光标在Untracked files这一行, 按TAB, 即可将UnTracked files这个小节给折叠起来, 工程文件多的时候这个功能十分管用. 下面是将Recent commits折叠后的结果. C-p, C-n \u0026 M-p, M-n: C-p和C-n是上下移动, 与emacs移动相同. M-p和M-n是同级上下移动. 比如上面这个图, 如果光标在file这一行, 按M-n则会移动到new这一行, 如果光标在Untracked files这一行, 按M-n则会移动到Unstaged changes这一行. d d: 查看diff, 将text.txt文件中的内容修改, 并将光标移动到Unstaged changes中的 modified text.txt 这一行, 按两下d, 即可弹出 diff 相关内容, 弹窗如下: 这个弹窗分两部分, Unstaged changes部分是文件名以及其修改的总结. 它的下面是针对每个文件高亮修改的内容. 可以清楚地看到改支的行以及改动的内容: 红删除, 蓝为添加行, 标红内容为删除内容, 标蓝内容为添加内容. 一开始看可能有些不习惯, 但习惯后这个功能十分有用. 此外, 如果在某一个提交行按 d d, 则会显示本次提交与前一次提交的diff s \u0026 u: s可将文件添加到暂存区, u可将文件移出暂存区. 比如光标调整到new这个文件上, 按s后结果为: 可以看到new这个文件从Untracked files移到了staged changes这个小节下. 翻译成git的话就是 使用了 git add new 这个命令. q: 退出当前buffer. Enter: 到处都能用, 一个很好用的地方是, 在查看diff时, 在某一行按Enter, 它会直接跳到这个文件的这一行供你修改. c: 提交弹窗, 如下: 此时按c, 即Commit (git commit -m), (通常直接按两下c), 这时会进入提交信息提示弹窗, 输入完提示信息后 再按C-c C-c 即可完成提交 (C-c C-k 放弃提交). 此时按a, 即 git commit --amend l: log弹窗 r: rebase弹窗 ","date":"2022-08-15","objectID":"/posts/org_05_magit_usage/:2:2","series":null,"tags":["git"],"title":"Magit的基本使用","uri":"/posts/org_05_magit_usage/#常用的一些快捷键"},{"categories":["额外学习"],"content":"可能看起来学习成本高, 但其实学会后好处要远远大于学习成本 ","date":"2022-08-15","objectID":"/posts/org_05_magit_usage/:3:0","series":null,"tags":["git"],"title":"Magit的基本使用","uri":"/posts/org_05_magit_usage/#可能看起来学习成本高-但其实学会后好处要远远大于学习成本"},{"categories":null,"content":"基础(最常用的命令) git init #初始化Git仓库 git add . #添加所有未追踪文件或修改 git add filename #添加指定文件 git commit -m \"commit information\" # 提交 git push #推送到远程 git pull #从远程拉取 git status # 查看当前状态 git log # 查看提交日志 ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:1:0","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#基础--最常用的命令"},{"categories":null,"content":"版本回退 前一个版本: I’m a little tired!!! 当前版本: I’m not tired!!! 这时使用 git add 和 git commit 提交了, 但是想回退过去的操作办法如下: git中HEAD表示当前的提交版本, HEAD^表示前一个提交版本,所以想回退的操作为: git reset --hard HEAD^ 未回退命令时日志显示: 有两个版本: first 和 not tired 使用回退命令后日志显示: 只有 first 了 当知道commit ID时, 使用 commit ID直接跳到想要的版本. 比如现在想再返回 not tired, 操作命令为: git reset --hard commit_id 这时再查看log结果为: 可以看到, 使用了这个命令后, 我们的not tired 提交又回来了. 当不知道commit ID时也是有办法的 git提供了 git reflog 命令, 这个命令的输出结果是记录你的每一次命令, 使用 git reflog 命令查看历史命令: 可以看到第一列记录了所有的commit ID, 最后一列记录了执行的操作, 根据最后一列信息找到对应的ID号即可 ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:2:0","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#版本回退"},{"categories":null,"content":"回退问题: 如何跳到某个指定commit_id的版本? 假如你刚提交了一个版本, 现在后悔了想返回到前一个版本, 应该使用什么命令? 假如你回退到前某个版本, 现在后悔了, 想返回到最近的某个版本, 但是 commit_id 不记得, 该怎么办? ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:2:1","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#回退问题"},{"categories":null,"content":"回退总结: HEAD 为当前版本, 使git reset --hard commit_id 可以跳到对应的版本 git reflog 命令可以查看所有操作的记录, 可以用于查找所需要的版本号 tip: HEAD表示当前版本, HEAD^表示前一个提交版本, HEAD^^为前2个提交的版本, HEAD~100 为第前100个版本 ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:2:2","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#回退总结"},{"categories":null,"content":"撤销修改 前一节所说的版本回退有个缺点, 那就是, 回退到前一个版本, 所有相关文件的最新修改均会丢失. 并且版本回退是针对已提交的内容的. 现在有个新问题: 在git管理的版本下, 撤销对某个文件的修改. 这之前要了解一下git管理的基本逻辑. ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:3:0","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#撤销修改"},{"categories":null,"content":"git 基本逻辑 git仓库其实分为工作区(working tree), 暂存区(stage), 和 仓库() 工作区就是我们看到和使用的部分 暂存区是我们使用 git add filename 后文件所到达的部分 仓库是我们使用 git commit 后文件所在的部分 在使用 git commit 命令时, 只会将暂存区部分的内容保存到仓库 ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:3:1","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#git-基本逻辑"},{"categories":null,"content":"文件修改的撤销 在上一次提交后, 本次我们对某个文件进行了修改, 发现修改后程序运行出错了. 现在删除对文件的修改, 应该怎么做(并未使用 git add命令)? 这时需要的命令为 git checkout -- filename. 对文件修改后查看工作区状态: 可以看到有修改, 这时使用 git checkout filename 后结果为: 再次查看文件, 修改己经没有了 假设这次我们对文件进行了修改, 且已经使用了 git add filename了该怎么办呢? 办法是使用 git reset HEAD filename 记其回到未 add 的情况. 我们将修改添加到暂存区, 并查看状态: 先看一下与前面的区别, 未add时, 字是红色的, 提示信息是\"尚未暂存以备提交的变更\" 使用 git add filename 后提示信息是\"要提交的变更\" 此时使用 git reset HEAD filename 后的状态为: 可以看到取消了暂存的变更, 这时查看状态则又到了修改未提交的状态了. 此时再使用 git checkout -- filename 命令即可以撤销修改了. ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:3:2","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#文件修改的撤销"},{"categories":null,"content":"撤销问题 有修改, 但是未添加到暂存区, 如何撤销? 有修改, 已添加到暂存区, 如何撤销? ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:3:3","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#撤销问题"},{"categories":null,"content":"撤销总结 git checkout -- filename 撤销对工作区文件的修改 git reset HEAD filename 撤销对文件的暂存操作 ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:3:4","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#撤销总结"},{"categories":null,"content":"文件删除 git仓库的文件删除和正常文件夹的删除不大一样, 因为git将删除也视为了一个操作, 因此删除也需要提交. 正常删除文件后查看仓库状态: 同样, 使用 git checkout -- filename也可以取消该操作. 使用 git rm filename 删除文件后查看状态: 可以看到, 直接删除后提示信息是红色的, 且提示信息是\"修改尚未加入提交\". 也就是说, 这时还需要使用 git add命令才可以使用 git commit将删除操作提交到仓库. 而 git rm filename则可以直接将删除提交到仓库. 对于已添加,未添加的修改如何撤销可见上一节. ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:4:0","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#文件删除"},{"categories":null,"content":"删除问题 若要删除仓库中的文件应如何操作? 直接删除与使用 git rm 命令删除有何不同? 删除后如何撤销? ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:4:1","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#删除问题"},{"categories":null,"content":"删除总结 删除与修改一样, 都是要提交到仓库的 使用 git rm 操作相当于使用 直接删除并使用了 git add 命令 删除的撤销可根据是否添加到暂存区同修改一样操作 ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:4:2","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#删除总结"},{"categories":null,"content":"分支 ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:5:0","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#分支"},{"categories":null,"content":"创建与合并 分支的创建与删除: git branch branch_name 创建新分支前: 创建新分支后: git branch newBranch (* 表示当前HEAD所在的分析, 即当前工作区的分支) 切换分支: git checkout newBranch 可以看到 * 从main到了newBranch 分支删除: git branch -d newBranch (不能删除当前所在分支, 因此要先切回main分支才可执行该操作) 分支的合并现有两个分支:main和bugFix 合并前: 将HEAD放在main分支上, 并执行 git merge bugFix: 可以看到, 合并会产生一个新的提交, 且当前节点有了两个父节点. 这里其实可以更深入理解下git checkout这个命令, 它的真实作用是切换当前工作区在git这个仓库树中的位置.即, HEAD可以指向main或者bugFix这种具体的分支名, 也可以指向某一个commit_id(每一个commit_id即是工作区的一个版本) ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:5:1","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#创建与合并"},{"categories":null,"content":"冲突的修正 假如我们现在有2个分支: master和newBranch, 在这两个分支中我们都对一个文件进行了修改. 这时, 将newBranch分支合并到master分支时就会出现冲突: 这里master分支中我们添加的内容是\"Creating a new branch!!!“并提交了. 在newBranch分支中我们添加的内容是\"Create a new branch!!!”. 所以在HEAD处于master分支, 并运行 git merge newBranch时会出现冲突. 我们打开提示的冲突文件, 文件内容如下: Git用\u003c，=，\u003e标记出不同分支的内容，我们修改如下后保存： 这时查看仓库状态为: 此时将其添加到仓库并提交 这时查看日志树可以看到合并过程: ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:5:2","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#冲突的修正"},{"categories":null,"content":"分支问题 如何创建分支与删除分支? 如何在不同分支中进行切换? 如何合并不同的分支? 合并分支后有冲突了该怎么办? ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:5:3","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#分支问题"},{"categories":null,"content":"分支总结 分支创建: git branch branch_name 分支删除: git branch -d branch_name 将其他分支合并到当前分支: git merge other_branch 分支切换: git checkout branch_name 冲突解决: 修正冲突的文件并添加到暂存区, 然后提交 ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:5:4","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#分支总结"},{"categories":null,"content":"高级篇:命令详解 ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:6:0","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#高级篇-命令详解"},{"categories":null,"content":"分离HEAD 这是一个很小的概念, 单独说它, 就是因为它很重要. 其实git是由不同的提交组成, 每个提交都可以看成一个树中的节点. 所有的提交在一起组成了一个提交树. 而当前的工作区(也就是我们能看到并修改的那些文件), 就是HEAD所指向的. 其实HEAD并不是非要指向具体的分支, 它也可以直接指向某个提交. 比如当前的提交树为: 可以看到*在bugFix这个分支上, 这也表示HEAD指向bugFix, 我们可以直接让它指向C4 git checkout C4 (这里的C4是提交id): 同样, 也可以使用这个命令让它指向 main, 或者 C0: 其实checkout这个命令用来切换HEAD的指向并不准确, 新版的git使用switch命令 ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:6:1","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#分离head"},{"categories":null,"content":"相对引用(^) 在不同的仓库进行切换时, 使用commit id十分麻烦, 因为id需要使用git log命令查找, git提供了相对引用, 即^表示上一个, ^^表示上2个, ~10 表示前10个. 如使用 git checkout main^ 效果为: 可以看到当HAED指向main时, HEAD^和main^都是指向C5的. 再次使用 git checkout HEAD~3 结果为: ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:6:2","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#相对引用"},{"categories":null,"content":"撤销变更 (reset \u0026 revert) 当本地出错要回退时, 有2个命令可用. reset可将当前分支回退到指定提交. 可看执行结果: 原本提交树是这样, 使用 git reset HEAD^ 后: 可以看到, 这个命令让分支放弃了当前节点而指向了父节点. 若对原本的提交树使用 git revert HEAD^ 命令后的提交树为: 可以看到, 现在这人分支指向了C1’这个提交, 其实C1’与C1是相同的, 但是 git 将回退作为一个操作然后提交到仓库. 这样做的好处: 在本地其实是没有什么区别的, 但是如果推送到远程, 使用git reset后若要与远程同步, 需要将远程的最新提交删除, 这个操作难以实现, 因此将回退作为一个新的提交可以方便远程与本地的同步, 方便多人合作. ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:6:3","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#撤销变更--reset-and-revert"},{"categories":null,"content":"提交树的节点移动 (rebase \u0026 cherry-pick) rebase: 它的主要作用是创建线性提交记录, 方便工程修改脉路整理. 比如对于当前提交树: 我们使用 git rebase main后提交树变化为: 现在分析一下这个结果. feature 和 main 分支的公共父节点是C1, 我在的HEAD指向feature, 此时运行以上命令后, git 将C2,C4 提交接到了main所指向的C3上, 也就是说: 这个命令会将当前分支的所有提交接到指定的分支上 因此 rebase 可以表象地理解为 剪枝 + 嫁接 如果使用merge命令的话提交树会变为: 这时提交记录则不够线性, 不方便查看对工程的修改历史 rebase 的另一个功能: 提交记录排序与删除. git rebase -i HEAD~4: 这个命令会弹出一个交互式的创口, 让你调整最新4个提交节点的顺序或者删除提交节点. 下面是一个例子: 知道这个功能即可, 不具体说明了 cherry-pick: 这个命令允许我们随意地移动提交树中的节点, 将指定的提交节点移动到当前的HEAD所指定的节点下. 对以下提交树: 使用 git cherry-pick C2 C4 C6 后结果为: 可以看到, 选中的提交节点都添加到了当前分支 main 所指向的节点下. 这样的命令有什么作用呢? 场景一: 当前出了一些问题, 可此需要新建一些分支来解决, 最终解决了当前的问题: 但是此时, 我们只想要收录解决问题的提交到当前的main下, 此时就可以使用 rebase -i 和 cherry-pick 这两个命令来实现. 将HEAD调整到main分支上, 再使用 git cherry-pick C4 (左) 或者使用 git rebase -i HEAD~3 后调整提交 (右) 即可获得: 场景二: 更新历史提交节点中的信息 下面的提交树中, 我想更新newImage提交中的信息, 且是只更新它的信息, 可使用 rebase -i 和 cherry-pick 这两种方式来实现. 口述使用 rebase -i 命令过程: 先调整C2和C3顺序,将C2提交节点放到最前, 再使用 git commit --amend 来提交列新C2中的信息, 最后使用 rebase -i 命令将C3节点放到最前. 详细说明 cherry-pick 过程 (因为不用交互, 易于博客中展示). 首先调整HEAD到main: git checkout main 其次, 将C2 拿到最前: git cherry-pick C2 然后, 修改C2中的内容并提交: git commit --amend 最后, 将C3节点放到最前: git cherry-pick C3 ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:6:4","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#提交树的节点移动--rebase-and-cherry-pick"},{"categories":null,"content":"提交树的节点移动 (rebase \u0026 cherry-pick) rebase: 它的主要作用是创建线性提交记录, 方便工程修改脉路整理. 比如对于当前提交树: 我们使用 git rebase main后提交树变化为: 现在分析一下这个结果. feature 和 main 分支的公共父节点是C1, 我在的HEAD指向feature, 此时运行以上命令后, git 将C2,C4 提交接到了main所指向的C3上, 也就是说: 这个命令会将当前分支的所有提交接到指定的分支上 因此 rebase 可以表象地理解为 剪枝 + 嫁接 如果使用merge命令的话提交树会变为: 这时提交记录则不够线性, 不方便查看对工程的修改历史 rebase 的另一个功能: 提交记录排序与删除. git rebase -i HEAD~4: 这个命令会弹出一个交互式的创口, 让你调整最新4个提交节点的顺序或者删除提交节点. 下面是一个例子: 知道这个功能即可, 不具体说明了 cherry-pick: 这个命令允许我们随意地移动提交树中的节点, 将指定的提交节点移动到当前的HEAD所指定的节点下. 对以下提交树: 使用 git cherry-pick C2 C4 C6 后结果为: 可以看到, 选中的提交节点都添加到了当前分支 main 所指向的节点下. 这样的命令有什么作用呢? 场景一: 当前出了一些问题, 可此需要新建一些分支来解决, 最终解决了当前的问题: 但是此时, 我们只想要收录解决问题的提交到当前的main下, 此时就可以使用 rebase -i 和 cherry-pick 这两个命令来实现. 将HEAD调整到main分支上, 再使用 git cherry-pick C4 (左) 或者使用 git rebase -i HEAD~3 后调整提交 (右) 即可获得: 场景二: 更新历史提交节点中的信息 下面的提交树中, 我想更新newImage提交中的信息, 且是只更新它的信息, 可使用 rebase -i 和 cherry-pick 这两种方式来实现. 口述使用 rebase -i 命令过程: 先调整C2和C3顺序,将C2提交节点放到最前, 再使用 git commit --amend 来提交列新C2中的信息, 最后使用 rebase -i 命令将C3节点放到最前. 详细说明 cherry-pick 过程 (因为不用交互, 易于博客中展示). 首先调整HEAD到main: git checkout main 其次, 将C2 拿到最前: git cherry-pick C2 然后, 修改C2中的内容并提交: git commit --amend 最后, 将C3节点放到最前: git cherry-pick C3 ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:6:4","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#场景一-当前出了一些问题-可此需要新建一些分支来解决-最终解决了当前的问题"},{"categories":null,"content":"提交树的节点移动 (rebase \u0026 cherry-pick) rebase: 它的主要作用是创建线性提交记录, 方便工程修改脉路整理. 比如对于当前提交树: 我们使用 git rebase main后提交树变化为: 现在分析一下这个结果. feature 和 main 分支的公共父节点是C1, 我在的HEAD指向feature, 此时运行以上命令后, git 将C2,C4 提交接到了main所指向的C3上, 也就是说: 这个命令会将当前分支的所有提交接到指定的分支上 因此 rebase 可以表象地理解为 剪枝 + 嫁接 如果使用merge命令的话提交树会变为: 这时提交记录则不够线性, 不方便查看对工程的修改历史 rebase 的另一个功能: 提交记录排序与删除. git rebase -i HEAD~4: 这个命令会弹出一个交互式的创口, 让你调整最新4个提交节点的顺序或者删除提交节点. 下面是一个例子: 知道这个功能即可, 不具体说明了 cherry-pick: 这个命令允许我们随意地移动提交树中的节点, 将指定的提交节点移动到当前的HEAD所指定的节点下. 对以下提交树: 使用 git cherry-pick C2 C4 C6 后结果为: 可以看到, 选中的提交节点都添加到了当前分支 main 所指向的节点下. 这样的命令有什么作用呢? 场景一: 当前出了一些问题, 可此需要新建一些分支来解决, 最终解决了当前的问题: 但是此时, 我们只想要收录解决问题的提交到当前的main下, 此时就可以使用 rebase -i 和 cherry-pick 这两个命令来实现. 将HEAD调整到main分支上, 再使用 git cherry-pick C4 (左) 或者使用 git rebase -i HEAD~3 后调整提交 (右) 即可获得: 场景二: 更新历史提交节点中的信息 下面的提交树中, 我想更新newImage提交中的信息, 且是只更新它的信息, 可使用 rebase -i 和 cherry-pick 这两种方式来实现. 口述使用 rebase -i 命令过程: 先调整C2和C3顺序,将C2提交节点放到最前, 再使用 git commit --amend 来提交列新C2中的信息, 最后使用 rebase -i 命令将C3节点放到最前. 详细说明 cherry-pick 过程 (因为不用交互, 易于博客中展示). 首先调整HEAD到main: git checkout main 其次, 将C2 拿到最前: git cherry-pick C2 然后, 修改C2中的内容并提交: git commit --amend 最后, 将C3节点放到最前: git cherry-pick C3 ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:6:4","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#场景二-更新历史提交节点中的信息"},{"categories":null,"content":"tag 前文中提交节点要么是使用提交ID标识的, 要么是使用分支名子标识的. 分支名字会被移动, 且有新的提交时分支名字也会移动到新的提交节点中, 提交ID太长,难记.因此需要一个为某个提交节点进行永久命各的办法: 它就是tag. 对于以下的提交树: 为不同的提交节点添加tag, 如运行命令: git tag V1 C1, git tag V2 C2, 这时的提交树为: 同时, 可以直接利用tag名称来移动HEAD: git checkout V1 可以看到这时HEAD指向了C1. 为什么不是V1* 呢, 这是因为无支对某个tag直接提交, 所以才会发生这种分离HEAD的现象. ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:6:5","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#tag"},{"categories":null,"content":"describe git describe \u003cref\u003e: \u003cref\u003e 为提交树中的引用, 默认为HEAD. 该命令的输出结果为: \u003ctag\u003e_\u003cnum\u003e_g\u003chash\u003e, tag为与该引用最近的标签, num 为 该引用与最近标签的间隔提交数, \u003chash\u003e为\u003cref\u003e的提交ID. 对于以下的提交树, 使用 git describe, git describe main, git describe side 的结果分别为: ","date":"2022-08-06","objectID":"/posts/org_04_git_usage/:6:6","series":null,"tags":["git"],"title":"Git的基本使用","uri":"/posts/org_04_git_usage/#describe"},{"categories":null,"content":"系列目的 介绍Git的基础配置及基础使用 介绍Magit的使用, 因为Magti相比与Git方便很多, 配合Emacs效率更高 ","date":"2022-08-06","objectID":"/posts/org_03_git_get_started/:1:0","series":null,"tags":["git"],"title":"Git 与 Magit 初始篇","uri":"/posts/org_03_git_get_started/#系列目的"},{"categories":null,"content":"使用命令行 windows 中使用bash, linux或Mac下使用terminal, 总之要开使用Git, 就要用命令行 要做的第一件事是检查电脑上是否安装了Git, 使用命令: git –version查看, 安装了则会返回版本号 没安装就去官网安装下 ","date":"2022-08-06","objectID":"/posts/org_03_git_get_started/:2:0","series":null,"tags":["git"],"title":"Git 与 Magit 初始篇","uri":"/posts/org_03_git_get_started/#使用命令行"},{"categories":null,"content":"配置Git的名字和邮箱 让Git知道你是谁, 两条命令: git config --global user.name \"your_name\" git config --global user.email \"your_email\" git config --list: #列出配置信息 git config --unset --global user.name # 取消名称设置 不使用 global 参数,则设置的是本仓库的信息 ","date":"2022-08-06","objectID":"/posts/org_03_git_get_started/:3:0","series":null,"tags":["git"],"title":"Git 与 Magit 初始篇","uri":"/posts/org_03_git_get_started/#配置git的名字和邮箱"},{"categories":null,"content":"配置Git代理 # 设置当前代理 git config http.proxy http://127.0.0.1:2334 # 取消当前代理 git config --unset http.proxy #设置socks5代理 git config http.proxy socks5://127.0.0.1 #取消全局代理 git config --global --unset http.proxy ","date":"2022-08-06","objectID":"/posts/org_03_git_get_started/:4:0","series":null,"tags":["git"],"title":"Git 与 Magit 初始篇","uri":"/posts/org_03_git_get_started/#配置git代理"},{"categories":null,"content":"初始化Git仓库 在需要使用Git管理的仓库中运行 git init 即可 ","date":"2022-08-06","objectID":"/posts/org_03_git_get_started/:5:0","series":null,"tags":["git"],"title":"Git 与 Magit 初始篇","uri":"/posts/org_03_git_get_started/#初始化git仓库"},{"categories":null,"content":"Magit 这是Emacs下的一个Git工具, 直接在Emacs下安装该包即可, 一般现有的Emacs工具都被作者安装好了. Emacsh下Magit的打开方式: M-x magit-status ","date":"2022-08-06","objectID":"/posts/org_03_git_get_started/:6:0","series":null,"tags":["git"],"title":"Git 与 Magit 初始篇","uri":"/posts/org_03_git_get_started/#magit"},{"categories":null,"content":"Git趣味学习 这个游戏可以学习基础； 这个网站可以助你熟练掌握命令； ","date":"2022-08-06","objectID":"/posts/org_03_git_get_started/:7:0","series":null,"tags":["git"],"title":"Git 与 Magit 初始篇","uri":"/posts/org_03_git_get_started/#git趣味学习"},{"categories":null,"content":"emacs: ox-hugo emacs里面主要是使用ox-hugo这个插件, 将所写的org文件转成md文件, 当然转换中仍有一些问题, 但勉强可以使用. ","date":"2022-08-04","objectID":"/posts/org_02_hugo_with_emacs/:1:0","series":null,"tags":["博客"],"title":"使用emacs写hugo博客","uri":"/posts/org_02_hugo_with_emacs/#emacs-ox-hugo"},{"categories":null,"content":"ox-hugo的安装 M-x list-packages: 这个命令会列出所有的可安装的包 C-s: 使用搜索找到ox-hugo 操控高亮到ox-hugo, 按i x 这两个字母, i表示Install, x表示eXecute. 在配置文件中添加ox-hugo配置 (use-package ox-hugo :ensure t ;Auto-install the package from Melpa :pin melpa ;`package-archives' should already have (\"melpa\" . \"https://melpa.org/packages/\") :after ox) 这时在org模式里就可以使用hugo了 C-c C-e时会多一个导出选项: 如果按以上操作, 有这一项出来, 那么ox-hugo才算成功, 没有就排查错误, 比如ox-hugo有没有安装上, 配置信息有没有加对地方 ","date":"2022-08-04","objectID":"/posts/org_02_hugo_with_emacs/:2:0","series":null,"tags":["博客"],"title":"使用emacs写hugo博客","uri":"/posts/org_02_hugo_with_emacs/#ox-hugo的安装"},{"categories":null,"content":"ox-hugo的使用 将org文件使用ox-hugo导出时,需要加一些配置才能导出到正确的位置.我参考这个配置: #+OPTIONS: author:nil ^:{} #+HUGO_FRONT_MATTER_FORMAT: YAML #+HUGO_BASE_DIR: 博客的根目录 #+HUGO_SECTION: posts/ #+HUGO_CUSTOM_FRONT_MATTER: :toc true #+HUGO_AUTO_SET_LASTMOD: t #+HUGO_TAGS: 标签1 标签2 #+HUGO_CATEGORIES: 类别 #+HUGO_DRAFT: false #+TITLE: 博客名字 这其实是一个重复性的工作, 因此使用Yasnippet模板 配置Yasnippet模版(安装使用部分略, 使用的Centaur Emacs贡献者已安装配置好了) 使用: M-x yas-new-snippet (C-c \u0026 C-n) 调出添加模板界面 添加以下内容, 再保存(起个名字:hugo, 这默认保存在~/.emacs.d/snippets/org-mode/hugo中) # -*- mode: snippet -*- # name: hugo_blog # key: \u003ehugo # -- #+OPTIONS: author:nil ^:{} #+hugo_front_matter_format: yaml #+HUGO_BASE_DIR: ../ #+HUGO_SECTION: posts/ #+DATE: `(format-time-string \"[%Y-%m-%d %a %H:%M]\")` #+HUGO_CUSTOM_FRONT_MATTER: :toc true #+HUGO_AUTO_SET_LASTMOD: t #+HUGO_TAGS: $1 #+HUGO_CATEGORIES: $2 #+HUGO_DRAFT: false #+TITLE: $3 模板解释: 1. key: 后面部分, 这个是所使用的快捷键；2. #–: 后面的部分是模板 这时输入在文中 \u003ehugo, 再按TAB后就可以弹出模板内容 #+OPTIONS: author:nil ^:{} #+hugo_front_matter_format: yaml #+HUGO_BASE_DIR: ../ #+HUGO_SECTION: posts/ #+DATE: [2022-08-05 Fri 00:41] #+HUGO_CUSTOM_FRONT_MATTER: :toc true #+HUGO_AUTO_SET_LASTMOD: t #+HUGO_TAGS: #+HUGO_CATEGORIES: #+HUGO_DRAFT: false #+TITLE: ","date":"2022-08-04","objectID":"/posts/org_02_hugo_with_emacs/:3:0","series":null,"tags":["博客"],"title":"使用emacs写hugo博客","uri":"/posts/org_02_hugo_with_emacs/#ox-hugo的使用"},{"categories":null,"content":"部署博客到github 在博客目录下运行 hugo 命令, 这是会更新根目录下public文件夹里的内容 申请一个仓库, 这个仓库是需要与用户名同名的:your_name.github.io 申请完后会有一些提示教你怎么去将本地仓库提交到github上 在本地的public目录下创建git目录 git init git add * git commit -m \"commit 1st\" git remote add origin https://github.com/your_name/your_name.github.io.git git push -u origin main 在提交的时候可能需要用到用户名your_name与密码,密码改为申请密钥 密钥申请: 点setting 选择developer settings 选择第三个 personal access tokens 点generate new token 对note, expiration 和 scopes进行设置即可. note是你这个token的名字, expiration是这个token的有效时间, select scopes是这个token的作用范围, 如果都不选这个token就一点用都没有 ","date":"2022-08-04","objectID":"/posts/org_02_hugo_with_emacs/:4:0","series":null,"tags":["博客"],"title":"使用emacs写hugo博客","uri":"/posts/org_02_hugo_with_emacs/#部署博客到github"},{"categories":null,"content":"自动推送脚本 hugo cd public git add . msg=\"rebuilding site `date`\" if [ $# -eq 1 ] then msg=\"$1\" fi git commit -m \"$msg\" git push cd .. 这里会需要输入github的用户名和密码,密码是前面创建的token 最后等仓库的action页面build完成就行了, 一般就是等一会 ","date":"2022-08-04","objectID":"/posts/org_02_hugo_with_emacs/:5:0","series":null,"tags":["博客"],"title":"使用emacs写hugo博客","uri":"/posts/org_02_hugo_with_emacs/#自动推送脚本"},{"categories":null,"content":"ubuntu 18.04安装hugo最新稳定版 在站上下载安装包 https://github.com/gohugoio/hugo/releases 安装即可 `sudo hugo version`查看版本 ","date":"2022-08-04","objectID":"/posts/org_01_hugo_install/:1:0","series":null,"tags":["博客"],"title":"hugo安装与入门","uri":"/posts/org_01_hugo_install/#ubuntu-18-dot-04安装hugo最新稳定版"},{"categories":null,"content":"创建hugo站 hugo new site hello-world 这个命令会创建一个hello-world文件夹 ","date":"2022-08-04","objectID":"/posts/org_01_hugo_install/:2:0","series":null,"tags":["博客"],"title":"hugo安装与入门","uri":"/posts/org_01_hugo_install/#创建hugo站"},{"categories":null,"content":"进入 hello-world文件夹 cd hello-world sudo hugo 即可显示提示信息, 若不这样显示, 就是出错了, 网上查别的教程吧 至此安装完成 ","date":"2022-08-04","objectID":"/posts/org_01_hugo_install/:3:0","series":null,"tags":["博客"],"title":"hugo安装与入门","uri":"/posts/org_01_hugo_install/#进入-hello-world文件夹"},{"categories":null,"content":"选择主题 hugo这个博客是需要配合现有的主题的, 我这里使用的是DoIt主题 ","date":"2022-08-04","objectID":"/posts/org_01_hugo_install/:4:0","series":null,"tags":["博客"],"title":"hugo安装与入门","uri":"/posts/org_01_hugo_install/#选择主题"},{"categories":null,"content":"主题下载 在这个网址下臷主题网址,我选择的是DoIt. DoIt的使用教程在这里 ","date":"2022-08-04","objectID":"/posts/org_01_hugo_install/:4:1","series":null,"tags":["博客"],"title":"hugo安装与入门","uri":"/posts/org_01_hugo_install/#主题下载"},{"categories":null,"content":"主题使用 下载主题并放到./hello-world/themes文件夹里 修改配置文件,就是根目录下的config.toml, 这里使用的是DoIt网站上提供的默认配置 baseURL = \"http://example.org/\" # [en, zh-cn, fr, ...] determines default content language defaultContentLanguage = \"en\" # language code languageCode = \"en\" title = \"My New Hugo Site\" # Change the default theme to be use when building the site with Hugo theme = \"DoIt\" [params] # DoIt theme version version = \"0.2.X\" [menu] [[menu.main]] identifier = \"posts\" # you can add extra information before the name (HTML format is supported), such as icons pre = \"\" # you can add extra information after the name (HTML format is supported), such as icons post = \"\" name = \"Posts\" url = \"/posts/\" # title will be shown when you hover on this menu link title = \"\" weight = 1 [[menu.main]] identifier = \"tags\" pre = \"\" post = \"\" name = \"Tags\" url = \"/tags/\" title = \"\" weight = 2 [[menu.main]] identifier = \"categories\" pre = \"\" post = \"\" name = \"Categories\" url = \"/categories/\" title = \"\" weight = 3 # Markup related configuration in Hugo [markup] # Syntax Highlighting (https://gohugo.io/content-management/syntax-highlighting) [markup.highlight] # false is a necessary configuration (https://github.com/dillonzq/LoveIt/issues/158) noClasses = false 打开hugo服务: hugo server (若要远程打开, 则命令为:hugo server –bind 0.0.0.0) 打开博客页面: http://localhost:1313 ","date":"2022-08-04","objectID":"/posts/org_01_hugo_install/:4:2","series":null,"tags":["博客"],"title":"hugo安装与入门","uri":"/posts/org_01_hugo_install/#主题使用"},{"categories":null,"content":"头像 先弄个需要作为头像的图片, 将其放到/static/images/avatar.jpg 在配置文件中添加(这里图片的URL相对目录是相对根目录下static或assert目录的) [params.home.profile] enable = true # Gravatar Email for preferred avatar in home page gravatarEmail = \"\" # URL of avatar shown in home page avatarURL = \"/images/avatar.jpg\" 将DoIt中/exampleSite/content文件夹中除posts以外的内容都复制到本博客的/content文件夹中(在此之前这个/content文件夹只有posts一个文件夹) 重新运行博客就有头像了 效果: ","date":"2022-08-04","objectID":"/posts/org_01_hugo_install/:4:3","series":null,"tags":["博客"],"title":"hugo安装与入门","uri":"/posts/org_01_hugo_install/#头像"},{"categories":null,"content":"hugo内容组织 保持博客文章存放在 content/posts 目录, 例如: content/posts/我的第一篇文章.md 保持简单的静态页面存放在 content 目录, 例如: content/about.md ","date":"2022-08-04","objectID":"/posts/org_01_hugo_install/:5:0","series":null,"tags":["博客"],"title":"hugo安装与入门","uri":"/posts/org_01_hugo_install/#hugo内容组织"},{"categories":null,"content":"本地资源 有三种方法来引用图片和音乐等本地资源: 使用页面包中的页面资源. 你可以使用适用于 Resources.GetMatch 的值或者直接使用相对于当前页面目录的文件路径来引用页面资源. 将本地资源放在 assets 目录中, 默认路径是 /assets. 引用资源的文件路径是相对于 assets 目录的. 将本地资源放在 static 目录中, 默认路径是 /static. 引用资源的文件路径是相对于 static 目录的. ","date":"2022-08-04","objectID":"/posts/org_01_hugo_install/:5:1","series":null,"tags":["博客"],"title":"hugo安装与入门","uri":"/posts/org_01_hugo_install/#本地资源"},{"categories":null,"content":"添加评论系统 因为博客托管在github上, 因此使用utterances评论系统, 这个评论系统使用的是github中的issue模块来存储评论内容, 因此要评论首先要登录github账号. 首先安装utterances, 点进这个网址, 选择你要的作为评论系统的repository(可以建个新的, 也可以用已有的, 但必须是public类型的repository), 记录下你的仓库名. 有人在这里提供了详细操作截图. 将仓库名写入配置文件 # DoIt NEW | 0.2.5 Utterances comment config [params.page.comment.utterances] enable = true # owner/repo repo = \"forrestk3/forrestk3.github.io\" issueTerm = \"title\" label = \"\" lightTheme = \"github-light\" darkTheme = \"github-dark\" 这里要修改的有两处: enable 后在改成 true；repo 后面改成你的用户名/仓库名, 这个仓库名就是前面在utterances安装时选择的仓库, 是需要授权的；其实issueTerm使用默认的也行, 但我觉得\"title\"类型更合理. 前面修改完后, 重新启动博客就可以在末尾看到我们添加的评论系统了, 效果如下, 因为我登录了, 所以能看到头像 这里需要注意的一点, 有的主题默认配置中有一项baseURL需要注释掉, 否则评论系统添加不成功(找了半天的错误) 当我们添加评论后, 相关项目的issue会出评论留存. 查看项目issue: ","date":"2022-08-04","objectID":"/posts/org_01_hugo_install/:6:0","series":null,"tags":["博客"],"title":"hugo安装与入门","uri":"/posts/org_01_hugo_install/#添加评论系统"},{"categories":["科研学习"],"content":" Shone N , Ngoc T N , Phai V D , et al. A Deep Learning Approach to Network Intrusion Detection[J]. IEEE Transactions on Emerging Topics in Computational Intelligence, 2018, 2(1):41-50. 本文提出用stacked Non-Symmetric deep Auto-encoders 方法进行特征抽取，并将提取的特征传给随机森林进行分类的一种入侵检测方法。 ","date":"2019-08-26","objectID":"/posts/12_dl_ids/:0:0","series":null,"tags":["论文阅读"],"title":"A Deep Learning Approach to Network Intrusion Detection","uri":"/posts/12_dl_ids/#"},{"categories":["科研学习"],"content":"基本知识介绍 ","date":"2019-08-26","objectID":"/posts/12_dl_ids/:0:0","series":null,"tags":["论文阅读"],"title":"A Deep Learning Approach to Network Intrusion Detection","uri":"/posts/12_dl_ids/#基本知识介绍"},{"categories":["科研学习"],"content":"当前网络安全的主要挑战 网络流量的激增 更深入和更细粒度监控的需求 协议的多种多样和数据的多元化 本文提出一种深度和浅度学习结合的方法：NDAE+RF，使用的测试数据集为KDD Cup'99和NSL-KDD。 ","date":"2019-08-26","objectID":"/posts/12_dl_ids/:1:0","series":null,"tags":["论文阅读"],"title":"A Deep Learning Approach to Network Intrusion Detection","uri":"/posts/12_dl_ids/#当前网络安全的主要挑战"},{"categories":["科研学习"],"content":"Auto-Encoder算法 ","date":"2019-08-26","objectID":"/posts/12_dl_ids/:0:0","series":null,"tags":["论文阅读"],"title":"A Deep Learning Approach to Network Intrusion Detection","uri":"/posts/12_dl_ids/#auto-encoder算法"},{"categories":["科研学习"],"content":"原算法 该算法有两个过程：Encoder和Decoder，Encoder过程为了让高维数据在低维中表示出来，Decoder过程将低维数据重新在高维中表示出来。 即它尝试去学习： $$h_{W,b} \\approx x \\tag{1}$$ h是非线性假设，W和b是权重和偏置。它的学习过程就是最小化损失函数： $$L(x,d(f(x))) \\tag{2}$$ 其中，L是损失函数，d是解码函数，f是编码函数 ","date":"2019-08-26","objectID":"/posts/12_dl_ids/:1:0","series":null,"tags":["论文阅读"],"title":"A Deep Learning Approach to Network Intrusion Detection","uri":"/posts/12_dl_ids/#原算法"},{"categories":["科研学习"],"content":"本文的Non-Symmetric Deep Auto-Envoder 多个隐藏层，且只有编码，没有解码 假设输入向量$x\\in R^d$,它将一步一步匹配隐藏层$h_i \\in R^{d_i}$，使用确定性函数表示如下： $$h_i=\\sigma(W_ih_{i-1}+b_i);i=1,\\dots,n \\tag{3}$$ 这里$h_0=x,\\sigma$是挤压函数，n是隐藏层数目。 NDAE没有decoder,它的输出向量类似于如下函数， $$y=\\sigma(W_{n+1}h_n+b_n+1) \\tag{4}$$ 模型可通过最小重构误差获得： $$E(\\theta)=\\sum_{i=1}^{m}(x^{(i)}-y^{(i)})^2 \\tag{5}$$ 这里有个东西它没讲请楚隐藏层的结点数与输入层不一样就没法算误差，如果一样就失去降维意义了 本文件所使用的stack-NDAE,结构如下，它是将两个NDAE接起来，并将结果传给RF进行分类。 ","date":"2019-08-26","objectID":"/posts/12_dl_ids/:2:0","series":null,"tags":["论文阅读"],"title":"A Deep Learning Approach to Network Intrusion Detection","uri":"/posts/12_dl_ids/#本文的non-symmetric-deep-auto-envoder"},{"categories":["科研学习"],"content":"评估和结果 实现：GPU Tensorflow,64-bit Ubuntu 16.04,Intel Xeon 3.60GH,16GB RAM,NVIDIA GTX 750 GPU ##数据集 KDD Cup'99和NSL-KDD,数据集组成情况如下表 高亮表示训练样例小于20个，在实验中被省略。 ","date":"2019-08-26","objectID":"/posts/12_dl_ids/:0:0","series":null,"tags":["论文阅读"],"title":"A Deep Learning Approach to Network Intrusion Detection","uri":"/posts/12_dl_ids/#评估和结果"},{"categories":["科研学习"],"content":"KDD CUP'99测试 评估5类分类性能，结果在表2中展示，可见我们的算法比DBN要好的多。 在我们的方法中，整体准确率为85.42%，优于DBN。对于R2L和U2R，由于测试集太少，因此准确率低。 性能比较在表3中： 我们的方法比DBN节省了78.19%的时间。 ","date":"2019-08-26","objectID":"/posts/12_dl_ids/:1:0","series":null,"tags":["论文阅读"],"title":"A Deep Learning Approach to Network Intrusion Detection","uri":"/posts/12_dl_ids/#kdd-cup99测试"},{"categories":["科研学习"],"content":"NSL-KDD测试 5类分类，结果比较在表4中，ROC曲线比较在图5中 13类分类结果比较见表5，训练时间比较见表6 ","date":"2019-08-26","objectID":"/posts/12_dl_ids/:2:0","series":null,"tags":["论文阅读"],"title":"A Deep Learning Approach to Network Intrusion Detection","uri":"/posts/12_dl_ids/#nsl-kdd测试"},{"categories":["科研学习"],"content":"未来工作：处理zero-day attack ","date":"2019-08-26","objectID":"/posts/12_dl_ids/:3:0","series":null,"tags":["论文阅读"],"title":"A Deep Learning Approach to Network Intrusion Detection","uri":"/posts/12_dl_ids/#未来工作处理zero-day-attack"},{"categories":["科研学习"],"content":" Garg S , Kaur K , Kumar N , et al. Hybrid Deep-Learning-Based Anomaly Detection Scheme for Suspicious Flow Detection in SDN: A Social Multimedia Perspective[J]. IEEE Transactions on Multimedia, 2019, 21(3):566-578. 本文利用受限波尔兹曼机进行异常检测。 ","date":"2019-08-25","objectID":"/posts/11_dl_ad/:0:0","series":null,"tags":["论文阅读"],"title":"Hybrid Deep-Learning-Based Anomaly Detection Scheme for Suspicious Flow Detection in SDN: A Social Multimedia Perspective","uri":"/posts/11_dl_ad/#"},{"categories":["科研学习"],"content":"异常检测模型 controller请求流统计信息，流收集模型收集并抽取特征，基于抽取的物征，利用改进的Restricted Boltzmann Machines进行降维，将结果交给本文提出的SVM算法中进行特征和行为分类，异常检测架构进行报告并通过安全通道发送给controller,controller根据报告进行更改流表和配置。 下图是本框架的方法论： ","date":"2019-08-25","objectID":"/posts/11_dl_ad/:0:0","series":null,"tags":["论文阅读"],"title":"Hybrid Deep-Learning-Based Anomaly Detection Scheme for Suspicious Flow Detection in SDN: A Social Multimedia Perspective","uri":"/posts/11_dl_ad/#异常检测模型"},{"categories":["科研学习"],"content":"A 降维：受限玻尔兹曼机（RBM） 设RBM有m个可见单元$v_i:[v_1,\\dots,v_m]$和n个隐藏单元$h_j:h=[h_1,\\dots,h_n]$。RBM的标准概率分布可表示为： $$P(h,v:\\theta)=\\frac{1}{Z(\\theta)}exp(a^Th+b^Tv+v^TWh) \\tag{1}$$ W表示权重，a,b表示偏置值，Z区分函数（也叫归一化因子）$\\theta=(W,a,b)$为模型参数。 改进：随机进行Dropout,实现方法：向量$r \\in {0,1}^n$p概率将其置1,将$r_j$关联到隐藏层的$h_j$,这时的联合概念分布为： $$P(r,h,v:p,\\theta)=P(r;p)\\mathcal{P}(h,v|r;\\theta) \\tag{2}$$ where. $$P(r;p)=\\sum_{j=1}^{n}p^{rj}(1-p)^{1-r_j} \\tag{3}$$ $$\\mathcal{P}=\\frac{1}{Z’(\\theta,r)}exp(a^Th+b^Tv+v^TWh) \\times \\sum_{j=1}^{n}g(h_j,r_j) \\tag{4}$$ 其中 $$f(x)=\\left{ \\begin{aligned} h_j=1;if\\ r_j = 1\\ h_j = 0;if\\ r_j=0 \\end{aligned} \\right. \\tag{5} $$ Z是归一化函数。经过Dropout后的RBM中h的激活概率： $$P(h|r,v)=\\sum_{j=1}^{n}P(h_j|r,v) \\tag{6}$$ $$P(h_j=1|r,v)=\\sigma(b_j+\\sum_{i=1}^{m}w_{i,j}v_i);if\\ r_j = 1 \\tag{7}$$ v的激活概率： $$P(v|h)=\\sum_{i=1}^{m}P(v_i|h) \\tag{8}$$ $$P(v_i=1|h)=\\sigma(a_i+\\sum_{j=1}^{n}w_{i,j}h_j) \\tag{9}$$ 当RBM训练完成后，它将输入向量转换为结果向量并传给SVM来进行更进一步的分类。 使用RBM进行降维算法： ","date":"2019-08-25","objectID":"/posts/11_dl_ad/:1:0","series":null,"tags":["论文阅读"],"title":"Hybrid Deep-Learning-Based Anomaly Detection Scheme for Suspicious Flow Detection in SDN: A Social Multimedia Perspective","uri":"/posts/11_dl_ad/#a-降维受限玻尔兹曼机rbm"},{"categories":["科研学习"],"content":"B. 分类：SVM 本SVM算法采用的核函数： $$k(x_j,x_k’)=w_1exp[\\frac{-||x_j-x’_k||^2}{2\\sigma ^2}]+w_2(1+x_j^Tx_k’)^d \\tag{10}$$ 该核函数结合了高斯核的局部性能和多项式核的合局性能。 随机梯度下降算法：利用该算法最小化损失函数$C(\\omega)$ $$\\omega_{t+1} \\leftarrow \\eta_t\\Delta_{\\omega}C(\\omega_t) \\tag{11}$$ 模型参数 使用Chaotic Differential Evolution处罚参数$C$,核函数中的$\\sigma$,权重系数$w_1,w_2$进行优化。 ","date":"2019-08-25","objectID":"/posts/11_dl_ad/:2:0","series":null,"tags":["论文阅读"],"title":"Hybrid Deep-Learning-Based Anomaly Detection Scheme for Suspicious Flow Detection in SDN: A Social Multimedia Perspective","uri":"/posts/11_dl_ad/#b-分类svm"},{"categories":["科研学习"],"content":"数据传送模型 分四个步骤： 数据提取 分类 报告 中继 控制平面利用异常检测模型将流量分为良性和恶性，将结果报告到控制平面，对恶性流量进行丢弃，良性流量进行继续中继。 ","date":"2019-08-25","objectID":"/posts/11_dl_ad/:0:0","series":null,"tags":["论文阅读"],"title":"Hybrid Deep-Learning-Based Anomaly Detection Scheme for Suspicious Flow Detection in SDN: A Social Multimedia Perspective","uri":"/posts/11_dl_ad/#数据传送模型"},{"categories":["科研学习"],"content":"多对象流路由框架（MoFR) MoFR提供了一个资源分布的最优化权衡 延迟最小化目标函数 $$\\mathbb{L}(\\delta_{k,v}(t))=\\left{ \\begin{aligned} \u0026(\\sum_j \\frac{d_{vi,vl}\\times \\delta_{k,vi}(t)\\times \\delta_{k,vl}(t)\\times a_{i,l}}{\\mathbb{P}r(t)})\\ \u0026+(\\sum_{i \\in w}\\sum_{j\\in|P(i)|}\\frac{\\mathcal{P}{i,j}(t)}{B{i,j}\\times O_{i,j}(t)})\\ \u0026+(\\sum_{i \\in w}\\sum_{j \\in |P(i)|}\\frac{|\\mathbb{Q}{ready}(t)|}{B{i,j} \\times O_{i,j}(t)})\\ \u0026+(\\sum_{i \\in w}\\mathbb{P}i(t)\\times \\sum_k(t_k^{end-t_k^{start}})\\times \\delta{k,vi(t)}) \\end{aligned} \\right.$$ $\\delta_{k,v}(t)$是一个二元值，表示switch v是否在第$l$个流中被使用，是则值为1,否则为0。 式中第一部分为传输时延，$d_{vi,vl}$表示第i和第l个switch距离(这里没讲清楚距离怎么计算),$\\mathbb{P}r(t)$表示传输时延中位数。 式中第二部分表示转换时延，$\\mathcal{P}{i,j}(t)$表示包大小，$O{i,j}(t)$表示占有率 式中第三部分表示查询时延，$|\\mathbb{Q}_{ready}(t)|$表示队列中的流数量 式中第四部分表示处理时延，$\\mathbb{P}_i(t)$表示由第i个节点的处理时延，$t_k^{start}$表示开始时间 2）带宽最大化函数 $$\\mathbb{B}(\\delta_{k,v}(t))=(\\sum_{i \\in w}\\sum_{j \\in |P(i)|}\\sum_k B_{i,j} \\times (t_k^{end} - t_k^{start})\\times \\delta_{k,vi}(t) \\times \\delta_{k,vl}(t)\\times a_{i,l})$$ 耗能最小化目标标函数 $$\\mathbb{E}(\\delta_{k,v}(t))=(F\\times \\sum_k(t_k^{end}-t_k^{start})\\times \\delta_{k,vi}(t))+(D\\times \\sum_k(t_k^{end}-t_k^{start})\\times \\delta_{k,vi}(t)\\times\\delta_{k,vl}(t)\\times a_{i,l})$$ 能耗由固定部分（风扇等）和动态部分组成（活动的端口数）F和D表示固定部分和静态部分所占的比例。 MoFR $$min\\mathbb{F}(\\delta_{k,v}(t))=f(-\\mathbb{B}(\\delta_{k,i}(t)),\\mathbb{L}(\\delta_{k,i}(t)),\\mathbb{E}(\\delta_{k,i}(t)))$$ ","date":"2019-08-25","objectID":"/posts/11_dl_ad/:1:0","series":null,"tags":["论文阅读"],"title":"Hybrid Deep-Learning-Based Anomaly Detection Scheme for Suspicious Flow Detection in SDN: A Social Multimedia Perspective","uri":"/posts/11_dl_ad/#多对象流路由框架mofr"},{"categories":["科研学习"],"content":"实现 （这里它并没有说是怎么实现的，只是说这么做，然后就给了个图） 1）实时数据集 该数据集是从Thapar Insitute of Engineering \u0026 Technology处获得。 观察异常流量所占比例图4a，比较RBM在使用Dropout和不使用的错误率图4b。 比较本文提出的方法，DRBM+GDSVM，RBM+GDSVM，RBM+SVM的侦测率图4c，正确率图4d，AUC图4e 比较SDN和传统网中延迟图4f，带宽图4g，能耗图4h 2）标准数据集KDD'99 比较我们的方法和其他人的方法的侦测率(DR),FPR,准确率，精确率，F值。 本文中方法对各种攻击的侦测率图5a，误判率图5b,准确率图5c，精确率图5d，F值图5e CMU insider threat dataset 对三个场景下与Deep Autoencoder进行性能比较 场景1：uncharacteristic behavior 场景2：steals confidential data 场景3:install keylogger to obtain the password ","date":"2019-08-25","objectID":"/posts/11_dl_ad/:0:0","series":null,"tags":["论文阅读"],"title":"Hybrid Deep-Learning-Based Anomaly Detection Scheme for Suspicious Flow Detection in SDN: A Social Multimedia Perspective","uri":"/posts/11_dl_ad/#实现"},{"categories":["科研学习"],"content":" Zeng H , Kazemian P , Varghese G , et al. Automatic test packet generation[C]// International Conference on Emerging Networking Experiments \u0026 Technologies. IEEE, 2012. ","date":"2019-08-24","objectID":"/posts/10_packet_generation/:0:0","series":null,"tags":["论文阅读"],"title":"Automatic test packet generation","uri":"/posts/10_packet_generation/#"},{"categories":["科研学习"],"content":"背景分析 当前特别需要自动化的网络测试工具 ","date":"2019-08-24","objectID":"/posts/10_packet_generation/:0:0","series":null,"tags":["论文阅读"],"title":"Automatic test packet generation","uri":"/posts/10_packet_generation/#背景分析"},{"categories":["科研学习"],"content":"概念 packets Switch Rules：defines how header space at ingress is transformed into regions of header space of egress. Rule Historyt:包被传送过程中记录自己所经历过的规则。 Topology:拓扑结构 ","date":"2019-08-24","objectID":"/posts/10_packet_generation/:0:0","series":null,"tags":["论文阅读"],"title":"Automatic test packet generation","uri":"/posts/10_packet_generation/#概念"},{"categories":["科研学习"],"content":"测试包生成算法过程 step1:生成一个all-pairs reachability table 举个例子： 图6所示，如果在$P_A$处注入all-x test packets,包会通过A传递，它会将10.0/16发送到B，将10.1/16发送到C，B再将10.0/16,tcp=80发送到$P_A$,C发送10.1/16到$P_C$，在表4第一行可见。 step2:抽样，抽取测试包，使每一个rule都会被至少一个包测试到。 step3:压缩，找到一个最小的测试包集，它们的rule history可以覆盖所有的rule(经典的最小集覆盖问题，可由贪心算法求得。) ","date":"2019-08-24","objectID":"/posts/10_packet_generation/:0:0","series":null,"tags":["论文阅读"],"title":"Automatic test packet generation","uri":"/posts/10_packet_generation/#测试包生成算法过程"},{"categories":["科研学习"],"content":"错误定位算法 ","date":"2019-08-24","objectID":"/posts/10_packet_generation/:0:0","series":null,"tags":["论文阅读"],"title":"Automatic test packet generation","uri":"/posts/10_packet_generation/#错误定位算法"},{"categories":["科研学习"],"content":"基本定义 定义：R(r,pk) = 1,(包pk在规则r下成功验证) 否则 R(r,pk)=0 包在规则下验证失败有两种情况：行为错误，包头匹配错误，这里只考虑行为错误（包被错误地处理）。我们只能在边缘观察到包，所以重新定义： $R(pk)=\\left{ \\begin{aligned} 0 \u0026\u0026 if\\ pk\\ fails\\ 1 \u0026\u0026 if\\ pk\\ succeeds \\end{aligned} \\right.$ ","date":"2019-08-24","objectID":"/posts/10_packet_generation/:1:0","series":null,"tags":["论文阅读"],"title":"Automatic test packet generation","uri":"/posts/10_packet_generation/#基本定义"},{"categories":["科研学习"],"content":"算法过程 假设1（错误传递）：R(pk) = 1 if and only if $\\forall r \\in pk.history,R(r,pk)=1$. 问题2（错误定位）：对于一个给定的元组列表：$(pk_0,R(pk_0),(pk_1,R(pk_1),\\dots$，找到所有的r，满足$\\exists pk_i,R(pk_i,r)=0$。 step1:找到可疑规则：所有通过的规则集P，失败包的规则集F，F-P即为可疑规则。 step2:缩小可疑规则集：重新发送所有包含可疑规则的测试（该包不在上述的测试包中）包，若通过，则将该规则移出可疑规则集。 step3:获得更小的可疑集后，报告可疑集。 ","date":"2019-08-24","objectID":"/posts/10_packet_generation/:2:0","series":null,"tags":["论文阅读"],"title":"Automatic test packet generation","uri":"/posts/10_packet_generation/#算法过程"},{"categories":["科研学习"],"content":"测试用例 ","date":"2019-08-24","objectID":"/posts/10_packet_generation/:0:0","series":null,"tags":["论文阅读"],"title":"Automatic test packet generation","uri":"/posts/10_packet_generation/#测试用例"},{"categories":["科研学习"],"content":"功能测试 交付规则：包通过该规则，并从正确的端口离开 链路规则：包正确地通过链路，并且包头未被修改 丢弃规则：将这种测试包进行广播，若在接收终端接收到，则失败。 ","date":"2019-08-24","objectID":"/posts/10_packet_generation/:1:0","series":null,"tags":["论文阅读"],"title":"Automatic test packet generation","uri":"/posts/10_packet_generation/#功能测试"},{"categories":["科研学习"],"content":"性能测试 阻塞：对每一对终端进行延迟测量，若延迟大于某个阈值，则定位延迟链路。 可用带宽：对每一个链路，队列，或服务类构造测试包进行测试（iperf/netperf或其他测试方法），可用带宽不应低于某一阈值。 严格的优先级：利用测试包阻塞低优先级类，再测试高优先级类，其带宽不应变化。 ","date":"2019-08-24","objectID":"/posts/10_packet_generation/:2:0","series":null,"tags":["论文阅读"],"title":"Automatic test packet generation","uri":"/posts/10_packet_generation/#性能测试"},{"categories":["科研学习"],"content":"实现 ","date":"2019-08-24","objectID":"/posts/10_packet_generation/:0:0","series":null,"tags":["论文阅读"],"title":"Automatic test packet generation","uri":"/posts/10_packet_generation/#实现"},{"categories":["科研学习"],"content":"测试包生成 使用python编写测试包生成器生成All-pairs reachability包，并用Min-Set-Cover算法获得包含所有测试规则的最小测试包。 ","date":"2019-08-24","objectID":"/posts/10_packet_generation/:1:0","series":null,"tags":["论文阅读"],"title":"Automatic test packet generation","uri":"/posts/10_packet_generation/#测试包生成"},{"categories":["科研学习"],"content":"网络监控器 假设前提是网络上有测试代理，网络监控器构造测试包，并指导代理发送测试包，代理通过IP proto域或TCP/UDP端口号区分测试包。如果测试失败，则从保留包集中选择其他测试包来定位错误。 ","date":"2019-08-24","objectID":"/posts/10_packet_generation/:2:0","series":null,"tags":["论文阅读"],"title":"Automatic test packet generation","uri":"/posts/10_packet_generation/#网络监控器"},{"categories":["科研学习"],"content":"可选实现 Cooperative routers:利用路由器发送测试包。 SDN-based testing:controller可以控制路由器发送测试包。 ","date":"2019-08-24","objectID":"/posts/10_packet_generation/:3:0","series":null,"tags":["论文阅读"],"title":"Automatic test packet generation","uri":"/posts/10_packet_generation/#可选实现"},{"categories":["科研学习"],"content":"evaluation 在i7,3.2GHz,6G memory，8线程运行ATPG工具，在stanford 和Internet2两个网络中进行测试。 规则重复率的累积分布函数： ","date":"2019-08-24","objectID":"/posts/10_packet_generation/:0:0","series":null,"tags":["论文阅读"],"title":"Automatic test packet generation","uri":"/posts/10_packet_generation/#evaluation"},{"categories":["科研学习"],"content":"在模拟网络中测试 使用mininet模拟网络，并使用仿真主机发送和接收由ATPG生成的测试包。 交付错误：手动替换交付规则观察结果 阻塞：限制链路30M/s，创建两个20M/s的UDP flow观察吞吐量，如图9左下，延迟如图9右下。 可用带宽：使用之前的链路，把UDP流降低至20M/s,并使用Pathload监控链路带宽，可以找到该链路瓶颈：bbra-yoza。 优先级：重复阻塞实验，当低优先级阻塞时，高优先级测试不受影响，高优先级阻塞时，均阻塞。 ","date":"2019-08-24","objectID":"/posts/10_packet_generation/:1:0","series":null,"tags":["论文阅读"],"title":"Automatic test packet generation","uri":"/posts/10_packet_generation/#在模拟网络中测试"},{"categories":["科研学习"],"content":" Peng Z, Xu S, Yang Z, et al. FOCES: Detecting Forwarding Anomalies in Software Defined Networks[C]// IEEE International Conference on Distributed Computing Systems. 2018. B类。本文提供一种在SDN中进行forwarding anomaly 检测方法，（只提供检测，并未提供定位与解决办法） ","date":"2019-08-23","objectID":"/posts/09_foces/:0:0","series":null,"tags":["论文阅读"],"title":"FOCES: Detecting Forwarding Anomalies in Software Defined Networks","uri":"/posts/09_foces/#"},{"categories":["科研学习"],"content":"基本思想 通过switch统计计数与controller推理计数相比较，判断是否有switch恶意转发包。 ","date":"2019-08-23","objectID":"/posts/09_foces/:0:0","series":null,"tags":["论文阅读"],"title":"FOCES: Detecting Forwarding Anomalies in Software Defined Networks","uri":"/posts/09_foces/#基本思想"},{"categories":["科研学习"],"content":"算法概念 假没有n个flows($f_1,f_2,\\cdots,f_n$)和m个rules($r_1,r_2,\\cdots,r_n$),定义流计数矩阵（FCM）$H_{m \\times n}$,其中，如果流满足规则，则对应矩阵值为1 $$ H_{i,j}=\\left{ \\begin{aligned} 1 \u0026\u0026 if\\ f_{i}\\ matches\\ r_i\\ 0 \u0026\u0026 otherwise \\end{aligned} \\right. \\tag{1} $$ rule计数向量$Y=(y_1,y_2,\\cdots,y_m)^T$,flow计数向量$X=(x_1,x_2,\\cdots,x_n)$ 由此可得FOCES: $$HX=Y \\tag{2}$$ 这是FOCES的基本依据，即：从switch处统计到的流计数与流容量和controller中计算获得的流计数相等。 假设流容量矩阵是$X_0$当switch出现交付异常时，FCM变成了$H’$,这时观察到的计数矩阵变成了$Y’=H’X_0 \\neq Y_0$。由于controller并不知道$H’$，所以当需要恢复$X_0$时就要利用 $$HX=Y’ \\tag{3}$$ 可以解得对$X_0$的估计 $$\\hat{X} = (H^TH)^{-1}H^TY’ \\tag{4}$$ 从而计算$\\hat{Y} = H\\hat{X}$,再计算绝对值差 $$\\Delta=|Y’-\\hat{Y}| \\tag{5}$$ 理想情况下，当绝对值差不为0时，则认为发生了交付异常。 有的情况下交付异常无法测出，如： $X_0=(3,4,5)^T$, , 可算得$\\hat{X}=(3,1,8)^T$,这时计算出的$\\Delta=0$，可以看到，这是一个反例，出现了交付异常却无法检测出来。这里我要把原文贴上了，因为我看不懂它说什么样的情况测不出来。 阈值计算方法： ","date":"2019-08-23","objectID":"/posts/09_foces/:0:0","series":null,"tags":["论文阅读"],"title":"FOCES: Detecting Forwarding Anomalies in Software Defined Networks","uri":"/posts/09_foces/#算法概念"},{"categories":["科研学习"],"content":"算法流程： 矩阵切片： 对于一个特定的switch Si,构造矩阵时只找与Si有关的规则，同样，也只计算流经Si的flow 切片算法流程： ","date":"2019-08-23","objectID":"/posts/09_foces/:0:0","series":null,"tags":["论文阅读"],"title":"FOCES: Detecting Forwarding Anomalies in Software Defined Networks","uri":"/posts/09_foces/#算法流程"},{"categories":["科研学习"],"content":"缺点 只能检测出有异常，而无法定位异常发生在哪个switch ","date":"2019-08-23","objectID":"/posts/09_foces/:0:0","series":null,"tags":["论文阅读"],"title":"FOCES: Detecting Forwarding Anomalies in Software Defined Networks","uri":"/posts/09_foces/#缺点"},{"categories":["科研学习"],"content":" Carvalho L F , Fernandes G , Rodrigues J J P C , et al. A novel anomaly detection system to assist network management in SDN environment[C]// IEEE International Conference on Communications. IEEE, 2017. C类。本文提出一种辅助网络管理的异常检测系统，算法运行在controller，分为四个模块： ","date":"2019-08-22","objectID":"/posts/08_ad_sdn/:0:0","series":null,"tags":["论文阅读"],"title":"A Novel Anomaly Detection System to Asist Network Management in SDN Enviroment","uri":"/posts/08_ad_sdn/#"},{"categories":["科研学习"],"content":"statistic collection module controller定期向switch发送收集信息请求，并以统计的形式将特征（如源/目的IP,源/目的port等）存入profile中$X={n_1,\\cdots,n_i,\\cdots,n_N}$，其中$n_i$表示特征i发生次数，这样便可获得熵的信息：$H(X)=-\\sum_(i=1)^N(\\frac{n_i}{S})log_2(\\frac{n_i}{S})$,其中，$S=\\sum_{i=1}^{N}n_i$ ","date":"2019-08-22","objectID":"/posts/08_ad_sdn/:0:0","series":null,"tags":["论文阅读"],"title":"A Novel Anomaly Detection System to Asist Network Management in SDN Enviroment","uri":"/posts/08_ad_sdn/#statistic-collection-module"},{"categories":["科研学习"],"content":"Anomaly Detection Module 本模块目的是找到异常流。首先，从profile里获得正常流信息，信息存放在$P_{n \\times a}$矩阵中，n是收集信息次数，a是信息维度，因此$p_{ij}$代表了第i个时间段的第j个特征。如果一个事件偏离profile太远，即被定义为异常，如果异常库没有该异常相关信息而将其存储并之后再检测。（这里并没有讲它是怎么计算偏移的，应该是通过熵）.有的常规流量也会有异常特征，这通过添加白名单来解决。 ","date":"2019-08-22","objectID":"/posts/08_ad_sdn/:0:0","series":null,"tags":["论文阅读"],"title":"A Novel Anomaly Detection System to Asist Network Management in SDN Enviroment","uri":"/posts/08_ad_sdn/#anomaly-detection-module"},{"categories":["科研学习"],"content":"Mitigation Module 检测到异常就将异常流信息发送至缓解模块，缓解模块将信息插入流表以禁止恶意流量交付。 ","date":"2019-08-22","objectID":"/posts/08_ad_sdn/:0:0","series":null,"tags":["论文阅读"],"title":"A Novel Anomaly Detection System to Asist Network Management in SDN Enviroment","uri":"/posts/08_ad_sdn/#mitigation-module"},{"categories":["科研学习"],"content":"Reporting Module 向管理员提供一些有用的信息，如未知异常。 本篇感觉总体感觉价值不大。 ","date":"2019-08-22","objectID":"/posts/08_ad_sdn/:0:0","series":null,"tags":["论文阅读"],"title":"A Novel Anomaly Detection System to Asist Network Management in SDN Enviroment","uri":"/posts/08_ad_sdn/#reporting-module"},{"categories":["科研学习"],"content":" Huijun P , Zhe S , Xuejian Z , et al. [J]. IEEE Access, 2018:1-1. 本文件基于knn的一个改进算法提出了一个异常检测算法。 ","date":"2019-08-21","objectID":"/posts/06_ad_sdn_acess/:0:0","series":null,"tags":["论文阅读"],"title":"A detection method for anomaly flow in software defined network","uri":"/posts/06_ad_sdn_acess/#"},{"categories":["科研学习"],"content":"一些概念： 欧几里德距离：$D_{ij}^y = \\sqrt{\\sum_{a=1}^{t}({X_{ia} - X_{ja}})^2}$ strangeness(陌生度)：$\\alpha_{iy}=\\frac{\\sum_{j=1}^kD_{ij}^y}{\\sum_{j=1}^kD_{ij}^{-y}}$ i与正例和反例的距离之比，越小越倾向与正例。 independence(独立度)：$\\theta_{iy}=\\sum_{j=1}^kD_{ij}^y $ i在正例中的距离之和，越小越倾向正例。 double p value:$p_1(\\alpha_i)=\\frac{\\#{j=(1,\\cdots,n):\\alpha_j \\ge \\alpha_i}}{n+1} $ 越大，则越正常 $p_2(\\theta_i) = \\frac{\\#{j=(1,\\cdots,n):\\theta_j \\ge\\theta_i}}{n+1} $ 越大，则越正常 ","date":"2019-08-21","objectID":"/posts/06_ad_sdn_acess/:0:0","series":null,"tags":["论文阅读"],"title":"A detection method for anomaly flow in software defined network","uri":"/posts/06_ad_sdn_acess/#一些概念"},{"categories":["科研学习"],"content":"算法步骤 step1 在训练集样例中进行欧几里德距离计算 step2 在训练集样例中进行陌生度和独立度计算 step3 计算检测点的陌生度和独立度 step4 计算检测点的double p value step5 识别异常点：结点为正常用结点条件为 $p_1(\\alpha_i) \\ge \\tau_1 \\quad and \\quad p_2(\\theta_i) \\ge \\tau_2$,$\\tau$由controller控制。 ","date":"2019-08-21","objectID":"/posts/06_ad_sdn_acess/:0:0","series":null,"tags":["论文阅读"],"title":"A detection method for anomaly flow in software defined network","uri":"/posts/06_ad_sdn_acess/#算法步骤"},{"categories":["科研学习"],"content":" Paxson, Vern. An analysis of using reflectors for distributed denial-of-service attacks[J]. ACM SIGCOMM Computer Communication Review, 2001, 31(3):38. 本文先讲解了DDoS攻击，再讲解基于reflector的DDoS攻击，最后基于各种网络协议的字段进行挨个分析，讲解哪些字段容易受到attacker的利用从而进行攻击。 DDoS攻击结构： 使用反射器的DDoS攻击 各协议可能被攻击的字段分析 ip ：Type of Service TCP:If the reflector’s stack has guessable TCP sequence numbers ICMP: reflectors generating ICMP messages can likely be filtered out. UDP:port number can be filtered. DNS:递归查询，欺骗查询 http: would be a significant threat were it not for the likely quick traceback due to the non-spoofed connection from the slave to the proxy. Definitely a significant threat if servers running on stacks with predictable sequence numbers are widely deployed. Other TCP application: would be a significant threat were it not for the likely quick traceback due to the non-spoofed connection from the slave to the proxy. Definitely a significant threat if servers running on stacks with predictable sequence numbers are widely deployed. 原文值得细看，不错的文章，就是语法太难了。 ","date":"2019-08-20","objectID":"/posts/07_reflector_sigcomm/:0:0","series":null,"tags":["论文阅读"],"title":"An Analysis of Using Reflectors for Distributed Denial-of-Service Attacks","uri":"/posts/07_reflector_sigcomm/#"},{"categories":["科研学习"],"content":" Kalkan K , Altay L , Gur G , et al. JESS: Joint Entropy Based DDoS Defense Scheme in SDN[J]. IEEE Journal on Selected Areas in Communications, 2018:1-1. 该文提出了一种基于熵的DDoS攻击的检测与缓解办法。 ","date":"2019-08-19","objectID":"/posts/04_jess/:0:0","series":null,"tags":["论文阅读"],"title":"JESS: Joint Entropy-Based DDoS Defense Scheme in SDN","uri":"/posts/04_jess/#"},{"categories":["科研学习"],"content":"introduction SDN存在安全问题，如DDoS攻击，解决办法有: 内在：structural attributes of SDN enviroment on the properties of traffic flows 外在：statistical and ml Entropy shows randomness,DDoS decraese randomness. ","date":"2019-08-19","objectID":"/posts/04_jess/:0:0","series":null,"tags":["论文阅读"],"title":"JESS: Joint Entropy-Based DDoS Defense Scheme in SDN","uri":"/posts/04_jess/#introduction"},{"categories":["科研学习"],"content":"related work 网络虚拟化及其安全 SDN安全相关研究 ","date":"2019-08-19","objectID":"/posts/04_jess/:0:0","series":null,"tags":["论文阅读"],"title":"JESS: Joint Entropy-Based DDoS Defense Scheme in SDN","uri":"/posts/04_jess/#related-work"},{"categories":["科研学习"],"content":"Join Entropy Based DDoS Defense Scheme In SDN(JESS) 分为三个小步： nominal stage:switch sends headers of all packets to controller,controller calculates joint entropies of each pair preparatory stage:controller generate current pair profiles and calculates joint entropies,difference exceeds $\\theta_j$,DDoS attack is detected,maximum difference is SuspiciousPair and send to switch active mitigation stage:switch creates SC of SuspiciousPair and send to controller,controller calculates correspoinding score and generates scoretable ST,rule table $RT_C$ with a designated $\\theta_s$,if score of entry under $\\theta_s$ forward, otherwise drop. 剩下的是算法的细则，太多了建议看原文 ","date":"2019-08-19","objectID":"/posts/04_jess/:0:0","series":null,"tags":["论文阅读"],"title":"JESS: Joint Entropy-Based DDoS Defense Scheme in SDN","uri":"/posts/04_jess/#join-entropy-based-ddos-defense-scheme-in-sdnjess"},{"categories":["科研学习"],"content":"Nominal Stage Nominal Profile Generation 收集一些重要的参数，表示成：$\\mathbb{P}={IP_{src},IP_{dst},P_{src},P_{dst},PROT,PKT_{size},TTC,TCP_{flag}}$,记$\\left(\\begin{aligned}\\mathbb{P}\\k\\end{aligned}\\right)$k个元组组成的集合，如$\\left(\\begin{aligned}\\mathbb{P}\\2\\end{aligned}\\right)={{IP_{src},IP_{dst}},\\dots,{TTL,TCP_{flag}}}$记$\\mathbb{A}i$是$\\left(\\begin{aligned}\\mathbb{P}\\2\\end{aligned}\\right)$的第i个元组，例$\\mathbb{A}1={IP{src},IP{dst}}$,它有$2^{32}*2^{32}$个可能。$\\mathbb{A}2={IP{src},P_{src}}$有$2^{32}*2^{16}$个可能。switch收集包头并发送给controller，收集到指定个数即可停。在指定包数被处理后假设有$\\alpha$个包对，则组成$\\alpha * k$矩阵，记为$E^{\\mathbb{A}_i}$,$E^{\\mathbb{A}_i}_j$表示为第j个元组，它的值为$E^{\\mathbb{A}_i}_j=（a_j,b_j）$。$E^{\\mathbb{A}_i}$可表示为： 基于$E^{\\mathbb{A}_i}$创建$N^{\\mathbb{A}_i}$，记为： 这里的$m_i$取决于这个阶段收集到不同属性的个数，这是一个$m \\times (k+1)$的矩阵。这里的建$N^{\\mathbb{A}_i}$就是Pair Nominal Profile，由controller形成。这里的$PAC^{\\mathbb{A}_i}$指的是${\\mathbb{A}_i}$的产生个数。 Nominal Profile of Order p 即p个历史阶段的Nominal Profile 第p个记为: 分阶段时，生成的profile为不同段的并集。用图表示如图4： Joint Entropy Calculation for Nominal Profiles 在生成profile阶段计算出该阶段的熵，并存入profile中，当检测时，将当时的流量计算出来的熵与profile中的熵比较，超出$\\theta_j$即可认定有DDoS发生，选相差最大的那个对作为主导对(determinant pair)。熵计算公式为： $$JN_{N^{A_i}}=-\\sum_{n=1}^{m_i}(\\frac{PAC^{A_i^n}}{\\alpha})\\log{(\\frac{PAC^{A_i^n}}{\\alpha})} \\tag{4}$$ 简记： $$PN_{A_i^n}=PAC^{A_i^n}/\\alpha \\tag{5}$$ 而式（4）变为： $$JN_{N^{A_i}}=-\\sum_{n=1}^{m_i}PN_{A_i^n}\\log{(PN_{A_i^n})} \\tag{6}$$ 归一化后最终得到的公式为： $$\\bar{JN}{N^{A_i}}=JN{N^{A_i}}/\\log[\\alpha] \\tag{7}$$ 使用时均用归一化的熵 ","date":"2019-08-19","objectID":"/posts/04_jess/:1:0","series":null,"tags":["论文阅读"],"title":"JESS: Joint Entropy-Based DDoS Defense Scheme in SDN","uri":"/posts/04_jess/#nominal-stage"},{"categories":["科研学习"],"content":"Preparatory Stage 本阶段检测攻击，并决定最适合的pair(这个对即用来识别异常的对)，分为三个阶段： Current Profile Generation 第i个当前的profile表示为： Joint Entropy Calculation for current profile 对当前profile而言， $$PC_{A_i^n}=PAC^{A_i^n}/\\alpha \\tag{9}$$ 则当前的熵JC表示为： $$JC_{N^{A_i}}=-\\sum_{n=1}^{m_i}PC_{A^n_i}\\log{(PC_{A_i^n})} \\tag{10}$$ Comparison $$\\Delta J_{N^{A_i}}=JC_{N^{A_i}}-JN_{N^{A_i}} \\tag{11}$$ 当这个值超过一个$\\theta_j$，则认为检测到了DDoS攻击， 对所有的差值，JESS找到差值最大的对，将其标为SuspiciousPair。 如：对$\\arrowvert\\mathbb{P}\\arrowvert = 8，k=2$时，$\\Delta J_{MAX}=\\max{\\Delta J_{N^{A_1}},\\Delta J_{N^{A_2}},\\cdots,\\Delta J_{N^{A_28}}}$,假设$\\Delta J_{N^{A_2}}$是最大的，就将其标为suspiciousPair，Controller便将这个SuspiciousPair发送给switch。 ","date":"2019-08-19","objectID":"/posts/04_jess/:2:0","series":null,"tags":["论文阅读"],"title":"JESS: Joint Entropy-Based DDoS Defense Scheme in SDN","uri":"/posts/04_jess/#preparatory-stage"},{"categories":["科研学习"],"content":"Active Mitigation Stage 这个过程有五个步骤 SuspiciousPair Profile Generation switch已知SuspiciousPair，则在本地生成Profile SC，并将其发送到controller Score Calculation: 对每个流计算$\\beta$: $$\\beta = \\frac{PC_{A_i^n}}{PN_{A_i^n}} \\tag{12}$$ 将计算所得的$\\beta$存于表ST中。 决定阈值 利用load shedding alporithm来决定，阈值$\\theta_S$由当前阈值$\\theta_C$和之前阈值$\\theta_P$共同决定 $$\\theta_S=\\frac{\\theta_C+\\theta_P}{2} \\tag{13}$$ Rule Generation 对每个SC中的entry进行计算，若打分超过阈值，drop it! otherwise,forwarded to the destination! Differing Rules($\\Delta - Rules$) determination 更新规则时，将当前规则与之前规则作对比，只发送不一样的那部分。 ","date":"2019-08-19","objectID":"/posts/04_jess/:3:0","series":null,"tags":["论文阅读"],"title":"JESS: Joint Entropy-Based DDoS Defense Scheme in SDN","uri":"/posts/04_jess/#active-mitigation-stage"},{"categories":["科研学习"],"content":"模拟与性能评估 数据集：MAWI 环境：RYU，8G RAM，Core i7-3610QM 2.3GHz CPU,Ubuntu 14.04 OS。 拓扑结构（图5）： （这个拓扑也太简单了点吧。。。） packet analysis in Mininet 利用mininet统计各种头信息，如表5 攻击种类 Performance Metrics 即评价标准：$FPR=\\frac{FP}{FP+TN}$ $ACC=\\frac{TP+TN}{TP+TN+FP+FN}$ Exprimental Results 结果分析 分析：误报率有时候会提高，原因是许多包有和攻击包一样的特征。为了提高效果，引入滑动窗口。 引入滑动窗口 网络拓扑为如图8 使用previous periods,也即意味着要存储这么多periods的所有的头值，如TCP滑动窗口般。（作 者经过计算，发现浪费不多 ）结果如图10,结论为：效果确实提高了不少！在不同攻击强度下效果对比 结果如图11所示： 结论：当攻击包与正常包数量相同时，难以区分。 算法分析 结论：时间复杂度：$O(n^2)$，处理加载过程不需要额外缓存；所需空间少，对controller负担小，通信代价几乎可勿略不计。 问题：为什么先择两个特征而不是更多个，没有做对比！ ","date":"2019-08-19","objectID":"/posts/04_jess/:0:0","series":null,"tags":["论文阅读"],"title":"JESS: Joint Entropy-Based DDoS Defense Scheme in SDN","uri":"/posts/04_jess/#模拟与性能评估"},{"categories":["科研学习"],"content":" Giotis K , Androulidakis G , Maglaris V . Leveraging SDN for Efficient Anomaly Detection and Mitigation on Legacy Networks[C]// Third European Workshop on Software Defined Networks. IEEE Computer Society, 2014. 本文主要提出一种架构：利用SDN中的Controller改进现有的网络体系中进行Anomaly Detection(AD) ","date":"2019-08-18","objectID":"/posts/03_sdn_ad/:0:0","series":null,"tags":["论文阅读"],"title":"Leveraging SDN for Efficient Anomaly Detection and Mitigation on Legacy Networks","uri":"/posts/03_sdn_ad/#"},{"categories":["科研学习"],"content":"Introduction DDoS的危害，以前的解决办法（文献较老）会使受害主要的良性流和恶性流量均不通过，本文的架构可改善这种状况。 ","date":"2019-08-18","objectID":"/posts/03_sdn_ad/:0:0","series":null,"tags":["论文阅读"],"title":"Leveraging SDN for Efficient Anomaly Detection and Mitigation on Legacy Networks","uri":"/posts/03_sdn_ad/#introduction"},{"categories":["科研学习"],"content":"Motivation and related word 讲述相关工作，但并未提及本文与之不同之处。（估记就是找几个不好的讲了一讲） ","date":"2019-08-18","objectID":"/posts/03_sdn_ad/:0:0","series":null,"tags":["论文阅读"],"title":"Leveraging SDN for Efficient Anomaly Detection and Mitigation on Legacy Networks","uri":"/posts/03_sdn_ad/#motivation-and-related-word"},{"categories":["科研学习"],"content":"Design Principles and Overall Architecture 本文提出的架构如图： ","date":"2019-08-18","objectID":"/posts/03_sdn_ad/:0:0","series":null,"tags":["论文阅读"],"title":"Leveraging SDN for Efficient Anomaly Detection and Mitigation on Legacy Networks","uri":"/posts/03_sdn_ad/#design-principles-and-overall-architecture"},{"categories":["科研学习"],"content":"A. Design pricinple 以流为粒度 data gathering,AD,mitigation function decoupling Dynamic triggering of the RTBH mechanism,remote and automatic configuation trigger device scalable traffic statistics collection using packet sampling techniques,achived by sFlow ","date":"2019-08-18","objectID":"/posts/03_sdn_ad/:1:0","series":null,"tags":["论文阅读"],"title":"Leveraging SDN for Efficient Anomaly Detection and Mitigation on Legacy Networks","uri":"/posts/03_sdn_ad/#a-design-pricinple"},{"categories":["科研学习"],"content":"B. overflow Architecture 见图1 有三个功能： Anomaly Detection/Identification two modules: first,statistics Collection,monitoring harvesting from the edge router,export them to the next module inline second,AD ,detect potential attck,identifies the victim,intructs the RTBH trigger device propagate static route RTBH Component match victim IP and redirect to OF switch propagate route to entir network Anomaly Mitigation identification malicious,segregate malicious and benign traffic,drop malicious traffic,fowarding benign traffic to inport. ","date":"2019-08-18","objectID":"/posts/03_sdn_ad/:2:0","series":null,"tags":["论文阅读"],"title":"Leveraging SDN for Efficient Anomaly Detection and Mitigation on Legacy Networks","uri":"/posts/03_sdn_ad/#b-overflow-architecture"},{"categories":["科研学习"],"content":"RTBH and anomaly detection empowered by the openflow protocol and sFlow capabilities ","date":"2019-08-18","objectID":"/posts/03_sdn_ad/:0:0","series":null,"tags":["论文阅读"],"title":"Leveraging SDN for Efficient Anomaly Detection and Mitigation on Legacy Networks","uri":"/posts/03_sdn_ad/#rtbh-and-anomaly-detection-empowered-by-the-openflow-protocol-and-sflow-capabilities"},{"categories":["科研学习"],"content":"A. Victim Identification Mechanism compute the average counter value and corresponding deviation,compare the value with particular IP ","date":"2019-08-18","objectID":"/posts/03_sdn_ad/:1:0","series":null,"tags":["论文阅读"],"title":"Leveraging SDN for Efficient Anomaly Detection and Mitigation on Legacy Networks","uri":"/posts/03_sdn_ad/#a-victim-identification-mechanism"},{"categories":["科研学习"],"content":"B. Using RTBH to redirect and filter network tarffic. (1) 以前的RTBH,会导致victim的benign traffic也不c通 (2) forward packets to OF-enable switch,drop only malicious traffic,enpowered the ADI with remotely configuring the RTBH trigger device ","date":"2019-08-18","objectID":"/posts/03_sdn_ad/:2:0","series":null,"tags":["论文阅读"],"title":"Leveraging SDN for Efficient Anomaly Detection and Mitigation on Legacy Networks","uri":"/posts/03_sdn_ad/#b-using-rtbh-to-redirect-and-filter-network-tarffic"},{"categories":["科研学习"],"content":"C. Anomaly Mitigation forward packets back inport of the OF,Drop malicious traffic. ","date":"2019-08-18","objectID":"/posts/03_sdn_ad/:3:0","series":null,"tags":["论文阅读"],"title":"Leveraging SDN for Efficient Anomaly Detection and Mitigation on Legacy Networks","uri":"/posts/03_sdn_ad/#c-anomaly-mitigation"},{"categories":["科研学习"],"content":"A distributed filtering mechanism against DDoS attacks: ScoreForCore Kalkan K, Alagöz F. A distributed filtering mechanism against DDoS attacks: ScoreForCore[J]. Computer Networks, 2016, 108: 199-209. 本文提出ScoreForCore基于主动和协作过滤的防御机制，特点为：在当前的攻击流中选择最合适的属性。本文由PackerScore[2]启发，由于[2]中的实时检测没有属性筛选，因此性能较差。 ","date":"2019-08-17","objectID":"/posts/01_scoreforcore/:0:0","series":null,"tags":["论文阅读"],"title":"A distributed filtering mechanism against DDoS attacks: ScoreForCore","uri":"/posts/01_scoreforcore/#a-distributed-filtering-mechanism-against-ddos-attacks-scoreforcore"},{"categories":["科研学习"],"content":"ScoreForCore 本文术语见表1 当前基于过滤的出防御方式有(individual,cooperative)X(reactive,proactive)，基中cooperative proactive为本文研究。 ","date":"2019-08-17","objectID":"/posts/01_scoreforcore/:0:0","series":null,"tags":["论文阅读"],"title":"A distributed filtering mechanism against DDoS attacks: ScoreForCore","uri":"/posts/01_scoreforcore/#scoreforcore"},{"categories":["科研学习"],"content":"Design issues 本文在[2]的基础上改进，使用[2]的打分方法。本文使用当前攻击最具代表性的属性创建profile。需要考虑如下： ","date":"2019-08-17","objectID":"/posts/01_scoreforcore/:1:0","series":null,"tags":["论文阅读"],"title":"A distributed filtering mechanism against DDoS attacks: ScoreForCore","uri":"/posts/01_scoreforcore/#design-issues"},{"categories":["科研学习"],"content":"profiling 分别在正常阶段和攻击阶段生成nominal 和current profile，要利用的属性有：（IP，Port,protocol type,packet size,TL,TCP flag。 nominal阶段本文采用路由随机选属性对来统计信息。因此每个路由存储每个属性的single profile和一个随机对属性：OwnPairs Nomial Profile(OPNP).攻击发生而本路由器没有最佳属性对时可从别的路由那获取。属性对profile示例如表2. 攻击阶段（阻塞发生时），对每个属性进行统计生成 single current profile(SingleCPs) ","date":"2019-08-17","objectID":"/posts/01_scoreforcore/:1:1","series":null,"tags":["论文阅读"],"title":"A distributed filtering mechanism against DDoS attacks: ScoreForCore","uri":"/posts/01_scoreforcore/#profiling"},{"categories":["科研学习"],"content":"comparison 将nominal profile与current profile相比较，如T3,T4,对每个属性找属性值统计中差最大的，如表3中的TCP和表4中的83，会找到6个属性值及其最大差值，再在这六个中找最大的前两个当作suspicious pair。对于当前路由器随机属性对中没有suspicious pair时，可从别的路由中获取。 ","date":"2019-08-17","objectID":"/posts/01_scoreforcore/:1:2","series":null,"tags":["论文阅读"],"title":"A distributed filtering mechanism against DDoS attacks: ScoreForCore","uri":"/posts/01_scoreforcore/#comparison"},{"categories":["科研学习"],"content":"collaboration 当前router没有，则向邻居请求可疑对的OPNP，并作为自己的SPNP。如果三跳邻居都没有，而使用自己的OPNP作为SPNP。 ","date":"2019-08-17","objectID":"/posts/01_scoreforcore/:1:3","series":null,"tags":["论文阅读"],"title":"A distributed filtering mechanism against DDoS attacks: ScoreForCore","uri":"/posts/01_scoreforcore/#collaboration"},{"categories":["科研学习"],"content":"score calculation 在前一步，每个router都有了SPCP和SPNP，对新包计算打分公式为： $S_{p}=\\frac{S \\operatorname{corePC} P_{(A=a_{p}, B=b_{p}\\right)} / T P C P}{\\text {ScorePNP}{\\left(A=a{p}, B=b_{p}, \\ldots)} / T P N P}$ ，TPCP为当前profile的总包数，TPNP为nomial profile的总包数 ","date":"2019-08-17","objectID":"/posts/01_scoreforcore/:1:4","series":null,"tags":["论文阅读"],"title":"A distributed filtering mechanism against DDoS attacks: ScoreForCore","uri":"/posts/01_scoreforcore/#score-calculation"},{"categories":["科研学习"],"content":"threshold calculation 阈值计算，作者的老套路，$CDF(Th)=\\Phi$ ","date":"2019-08-17","objectID":"/posts/01_scoreforcore/:1:5","series":null,"tags":["论文阅读"],"title":"A distributed filtering mechanism against DDoS attacks: ScoreForCore","uri":"/posts/01_scoreforcore/#threshold-calculation"},{"categories":["科研学习"],"content":"selective discarding 计算打分与阈值比，大的就丢，小的就交付，整个过程可见图2. （作者搞这么大个图显然就是在凑页数） ","date":"2019-08-17","objectID":"/posts/01_scoreforcore/:1:6","series":null,"tags":["论文阅读"],"title":"A distributed filtering mechanism against DDoS attacks: ScoreForCore","uri":"/posts/01_scoreforcore/#selective-discarding"},{"categories":["科研学习"],"content":"实验与评估 数据集：MAWI[17] 作者的老套路。 实验环境：C++编写，inter i5 3.3GHz,4G memory 拓扑：真实拓扑，如图3.有18个node，每个node平均有3个邻居 模型的攻击：还是老套路，见截图： ","date":"2019-08-17","objectID":"/posts/01_scoreforcore/:0:0","series":null,"tags":["论文阅读"],"title":"A distributed filtering mechanism against DDoS attacks: ScoreForCore","uri":"/posts/01_scoreforcore/#实验与评估"},{"categories":["科研学习"],"content":"性能评估 Metrics 结果见表5,6,7；表5中SFC的F值显然普遍高于PS，表6中SFC的TN值高于PS，表7中的SFC的准确率高于PS。 内存，通信代价，过滤效率的关系见表8 表中的APE为：attack prevention efficitncy,AY:accuracy 存储分析 理论主存储profile所要的最大的空间为300M，实际实验检测得只要几百K即可。 ","date":"2019-08-17","objectID":"/posts/01_scoreforcore/:1:0","series":null,"tags":["论文阅读"],"title":"A distributed filtering mechanism against DDoS attacks: ScoreForCore","uri":"/posts/01_scoreforcore/#性能评估"},{"categories":["科研学习"],"content":"Crowd-GPS-Sec: Leveraging Crowdsourcing to Detect and Localize GPS Spoofing Attacks Jansen K, Schäfer M, Moser D, et al. Crowd-GPS-Sec: Leveraging Crowdsourcing to Detect and Localize GPS Spoofing Attacks[C] IEEE Symposium on Security and\u003e Privacy (S\u0026P). 2018. ","date":"2019-08-16","objectID":"/posts/02_crowd-gps-sec/:0:0","series":null,"tags":["论文阅读"],"title":"Crowd-GPS-Sec: Leveraging Crowdsourcing to Detect and Localize GPS Spoofing Attacks","uri":"/posts/02_crowd-gps-sec/#crowd-gps-sec-leveraging-crowdsourcing-to-detect-and-localize-gps-spoofing-attacks"},{"categories":["科研学习"],"content":"Introduction ","date":"2019-08-16","objectID":"/posts/02_crowd-gps-sec/:0:0","series":null,"tags":["论文阅读"],"title":"Crowd-GPS-Sec: Leveraging Crowdsourcing to Detect and Localize GPS Spoofing Attacks","uri":"/posts/02_crowd-gps-sec/#introduction"},{"categories":["科研学习"],"content":"GPS ","date":"2019-08-16","objectID":"/posts/02_crowd-gps-sec/:0:0","series":null,"tags":["论文阅读"],"title":"Crowd-GPS-Sec: Leveraging Crowdsourcing to Detect and Localize GPS Spoofing Attacks","uri":"/posts/02_crowd-gps-sec/#gps"},{"categories":["科研学习"],"content":"A. GPS Usage in avaition 非常广泛 ","date":"2019-08-16","objectID":"/posts/02_crowd-gps-sec/:1:0","series":null,"tags":["论文阅读"],"title":"Crowd-GPS-Sec: Leveraging Crowdsourcing to Detect and Localize GPS Spoofing Attacks","uri":"/posts/02_crowd-gps-sec/#a-gps-usage-in-avaition"},{"categories":["科研学习"],"content":"B. GPS Spoofing Attacks 依赖GPS的航空工具都是攻击者倾向的目标 ","date":"2019-08-16","objectID":"/posts/02_crowd-gps-sec/:2:0","series":null,"tags":["论文阅读"],"title":"Crowd-GPS-Sec: Leveraging Crowdsourcing to Detect and Localize GPS Spoofing Attacks","uri":"/posts/02_crowd-gps-sec/#b-gps-spoofing-attacks"},{"categories":["科研学习"],"content":"1） Threat Model Spoffer is emulating a moving track such as a straight line or a curve with some potential acceleation ","date":"2019-08-16","objectID":"/posts/02_crowd-gps-sec/:2:1","series":null,"tags":["论文阅读"],"title":"Crowd-GPS-Sec: Leveraging Crowdsourcing to Detect and Localize GPS Spoofing Attacks","uri":"/posts/02_crowd-gps-sec/#1-threat-model"},{"categories":["科研学习"],"content":"2) Validation of assumptions first,position advertisements contain the spoofed GPS positions Second,spoofed signals will affect neighboring aircraft and UAVs GPS Spoofing Expriments The gole of these expriments is to demonstrate that existing transponders do not perform any checks on the derived GPS position and that spoffers can precisely control the positino and speed of victim receivers 实验表明欺骗很容易实现 GPS Spoofing Coverage Estimation 根据假设作出自由空间路径损失 $$L_{fs} = 32.45 + 20log_{10}(d_{km}) + 20 log_{10}(f_{MHz}) \\tag{1}$$ $L_{$fs}$自由空间路径损耗，$d_{km}$信号源与接收器距离，$f_{MHz}$信号频率 根据（1）得 $$Power -L_{fs}d - Attenuation \\geq -160[dBW]$$ 由该公式知所有离被攻击目标内34km的UAVs都会收到至少-160dBW的信号 ","date":"2019-08-16","objectID":"/posts/02_crowd-gps-sec/:2:2","series":null,"tags":["论文阅读"],"title":"Crowd-GPS-Sec: Leveraging Crowdsourcing to Detect and Localize GPS Spoofing Attacks","uri":"/posts/02_crowd-gps-sec/#2-validation-of-assumptions"},{"categories":["科研学习"],"content":"2) Validation of assumptions first,position advertisements contain the spoofed GPS positions Second,spoofed signals will affect neighboring aircraft and UAVs GPS Spoofing Expriments The gole of these expriments is to demonstrate that existing transponders do not perform any checks on the derived GPS position and that spoffers can precisely control the positino and speed of victim receivers 实验表明欺骗很容易实现 GPS Spoofing Coverage Estimation 根据假设作出自由空间路径损失 $$L_{fs} = 32.45 + 20log_{10}(d_{km}) + 20 log_{10}(f_{MHz}) \\tag{1}$$ $L_{$fs}$自由空间路径损耗，$d_{km}$信号源与接收器距离，$f_{MHz}$信号频率 根据（1）得 $$Power -L_{fs}d - Attenuation \\geq -160[dBW]$$ 由该公式知所有离被攻击目标内34km的UAVs都会收到至少-160dBW的信号 ","date":"2019-08-16","objectID":"/posts/02_crowd-gps-sec/:2:2","series":null,"tags":["论文阅读"],"title":"Crowd-GPS-Sec: Leveraging Crowdsourcing to Detect and Localize GPS Spoofing Attacks","uri":"/posts/02_crowd-gps-sec/#gps-spoofing-expriments"},{"categories":["科研学习"],"content":"2) Validation of assumptions first,position advertisements contain the spoofed GPS positions Second,spoofed signals will affect neighboring aircraft and UAVs GPS Spoofing Expriments The gole of these expriments is to demonstrate that existing transponders do not perform any checks on the derived GPS position and that spoffers can precisely control the positino and speed of victim receivers 实验表明欺骗很容易实现 GPS Spoofing Coverage Estimation 根据假设作出自由空间路径损失 $$L_{fs} = 32.45 + 20log_{10}(d_{km}) + 20 log_{10}(f_{MHz}) \\tag{1}$$ $L_{$fs}$自由空间路径损耗，$d_{km}$信号源与接收器距离，$f_{MHz}$信号频率 根据（1）得 $$Power -L_{fs}d - Attenuation \\geq -160[dBW]$$ 由该公式知所有离被攻击目标内34km的UAVs都会收到至少-160dBW的信号 ","date":"2019-08-16","objectID":"/posts/02_crowd-gps-sec/:2:2","series":null,"tags":["论文阅读"],"title":"Crowd-GPS-Sec: Leveraging Crowdsourcing to Detect and Localize GPS Spoofing Attacks","uri":"/posts/02_crowd-gps-sec/#gps-spoofing-coverage-estimation"},{"categories":["科研学习"],"content":"CROWD-GPS-SEC 所调查，传感器主要分布在美欧，我们的系统由三个模型组成 multilateration(MLAT) module estimates the location of airrcraft based on the time defference of Arrival(TDoA) between different sensors spoofing detection module check inconsistencies between multilaterated positions and GPS-derived positions in consistencies between position advertisements from different aircraft spoofer localization module has tetected a GPS spoofer estimates the position fo the spoffer. ","date":"2019-08-16","objectID":"/posts/02_crowd-gps-sec/:0:0","series":null,"tags":["论文阅读"],"title":"Crowd-GPS-Sec: Leveraging Crowdsourcing to Detect and Localize GPS Spoofing Attacks","uri":"/posts/02_crowd-gps-sec/#crowd-gps-sec"},{"categories":["科研学习"],"content":"A. Multilateration(MLAT) 每个被收到的信息可被表示成元组 $$ADS-B/Flarm Report :(\\hat a_i,t_s) \\tag{2}$$ 传播距离和TDoA的关系： $$dist(s_i,A) - dist(s_j,A) = \\Delta t_{i,j} \\tag{3} \\cdot c$$ $s_i$是i传感器的位置，A是发射信号的UAVs，$\\Delta t_{i,j}$为i,j的时间差,c光速。传感器距离交点即为UAVs的位置。 ","date":"2019-08-16","objectID":"/posts/02_crowd-gps-sec/:1:0","series":null,"tags":["论文阅读"],"title":"Crowd-GPS-Sec: Leveraging Crowdsourcing to Detect and Localize GPS Spoofing Attacks","uri":"/posts/02_crowd-gps-sec/#a-multilaterationmlat"},{"categories":["科研学习"],"content":"B. GPS Spoofing Detection 1) Time Alignment of Transmissions: 算出传递时间 $$t_{TX} = t_s - \\frac{dist(s,a)}{c} \\tag{4}$$ 利用相对时间计算UAVs的相对位置 $$ a_{REF} = \\left{ \\begin{aligned} \\frac{a_{TX} \\cdot(t_{TX+1} - t_{REF})+a_{TX+1} \\cdot (t_{REF} - t{TX})}{t_{TX+1} - t_{TX}}\u0026\u0026\u0026\u0026 t_{TX} \u003c t_{REF} \u0026 \\ a_{TX} \u0026\u0026\u0026\u0026 t_{TX} = t_{REF}\\ \\frac{a_{TX} \\cdot (t_{REF} - t_{TX-1}) + a_{TX-1} \\cdot (t_{TX} -t_{REF})}{t_{TX} - t_{TX-1}} \u0026\u0026\u0026\u0026 t_{TX} \u003e t_{REF} \\end{aligned} \\right. $$ (2) test1 (Cross-checks with MLAT) 测试位置差是否大于阈值 $$dist(a_i,\\hat a_i) \\overset{?}{\u003c} \\tau_i \\tag{5}$$ $a_i$是real position determined by MLAT,$\\hat a$ position reported by UAVs i using ADS-B/Flarm $\\tau$太大检测的成功率会减小，太小会提高出错率 复杂度：O(n) (3) test2 (Multiple Aircraft Comparison) UAVs间的距离比较是否小于阈值 $$dist(\\hat a_i,\\hat a_j) = d_{i,j} \\overset{?}{\u003e} \\tau _2 \\tag{6}$$ 复杂度：两两比较为$O(n^2)$，但通过最近邻算法可降到$n \\cdot Olog(n)$前面的n代表有n个UAVs。 （4）检测的时候两个test一起用 ##C. GPS Spoofer Localization ","date":"2019-08-16","objectID":"/posts/02_crowd-gps-sec/:2:0","series":null,"tags":["论文阅读"],"title":"Crowd-GPS-Sec: Leveraging Crowdsourcing to Detect and Localize GPS Spoofing Attacks","uri":"/posts/02_crowd-gps-sec/#b-gps-spoofing-detection"},{"categories":["科研学习"],"content":"1) localization Model 每个被欺骗的UAVs报告相同的位置，反射追踪这些位置找到欺骗者的位置。检测模块找到被 欺骗的UAVs，定位模块进行定位 ，将得到的变量代入公式。 $$dist(a_i,SP) - dist(a_j,SP) = d_{i,j} \\cdot \\frac{c}{v_{track}} \\tag{7}$$ 几何断点：三个双曲面有两个点，4个或更多就只有一个交叉点 要求：两个被欺骗的UAVs在不同位置即可得到4组乖式，进而得到解。 $$\\begin{equation} \\left ( \\begin{array}{ccc} m\\ 2 \\end{array} \\right ) \\end{equation} \\cdot t_s \\ge 4 \\tag{8}$$ Comparison with MLAT. 我们的方法更好，MLAT不行 ","date":"2019-08-16","objectID":"/posts/02_crowd-gps-sec/:2:1","series":null,"tags":["论文阅读"],"title":"Crowd-GPS-Sec: Leveraging Crowdsourcing to Detect and Localize GPS Spoofing Attacks","uri":"/posts/02_crowd-gps-sec/#1-localization-model"},{"categories":["科研学习"],"content":"2） Error Minimization: 用双曲面交叉来标记一个点而不是一个区域，错误函数为： $$E_t(SP,i,j) = dist(a_i,SP) - dist(a_j,SP) - d_{i,j} \\cdot \\frac{c}{v_{track}} \\tag{9}$$ root mean square error(RMSE)是 : $$\\begin{equation} \\mathop{\\arg\\min}{SP} \\sqrt{\\frac{\\sum{t = 1}^{\\infty}\\sum_{i = 1}^{m}\\sum_{j = 1}^{i - 1}E_t(SP,i,j)^2}{t \\cdot (\\frac{m^2}{2} - m)}} \\end{equation} \\tag{10}$$ 最小化损失函数即可最大准确率，被欺骗的UAVs越多结果越准确， ","date":"2019-08-16","objectID":"/posts/02_crowd-gps-sec/:2:2","series":null,"tags":["论文阅读"],"title":"Crowd-GPS-Sec: Leveraging Crowdsourcing to Detect and Localize GPS Spoofing Attacks","uri":"/posts/02_crowd-gps-sec/#2-error-minimization"},{"categories":["科研学习"],"content":"3） improved Filtering 对得到的位置进行正交化过滤 $$\\hat a_i - \\hat a_{j}^{’} \\bot tarck \\tag{11}$$ ","date":"2019-08-16","objectID":"/posts/02_crowd-gps-sec/:2:3","series":null,"tags":["论文阅读"],"title":"Crowd-GPS-Sec: Leveraging Crowdsourcing to Detect and Localize GPS Spoofing Attacks","uri":"/posts/02_crowd-gps-sec/#3-improved-filtering"},{"categories":null,"content":"  DoIt is a clean, elegant but advanced blog theme for Hugo developed by HEIGE-PCloud. It is based on the LoveIt Theme, LeaveIt Theme and KeepIt Theme. ","date":"2019-08-02","objectID":"/about/:0:0","series":null,"tags":null,"title":"About DoIt","uri":"/about/#"},{"categories":null,"content":"Features Performance and SEO  Optimized for performance: 99/100 on mobile and 100/100 on desktop in Google PageSpeed Insights  PJAX loading supported  Optimized SEO performance with a correct SEO SCHEMA based on JSON-LD  Google Analytics supported  Fathom Analytics supported  Baidu Analytics supported  Umami Analytics supported  Plausible Analytics supported  Search engine verification supported (Google, Bind, Yandex and Baidu)  CDN for third-party libraries supported  Automatically converted images with Lazy Load by lazysizes Appearance and Layout / Responsive layout / Light/Dark mode  Globally consistent design language  Pagination supported  Easy-to-use and self-expanding table of contents  Multilanguage supported and i18n ready  Beautiful CSS animation Social and Comment Systems  Gravatar supported by Gravatar  Local Avatar supported  Up to 64 social links supported  Up to 28 share sites supported  Disqus comment system supported by Disqus  Gitalk comment system supported by Gitalk  Valine comment system supported by Valine  Waline comment system supported by Waline  Facebook comments system supported by Facebook  Telegram comments system supported by Comments  Commento comment system supported by Commento  Utterances comment system supported by Utterances  Twikoo comment system supported by Twikoo  Vssue comment system supported by Vssue  Remark42 comment system supported by Remark42  giscus comment system supported by giscus Extended Features  Search supported by Lunr.js or algolia or Fuse.js  Twemoji supported  Automatically highlighting code  Copy code to clipboard with one click  Images gallery supported by lightgallery.js  Extended Markdown syntax for Font Awesome icons  Extended Markdown syntax for ruby annotation  Extended Markdown syntax for fraction  Mathematical formula supported by $ \\KaTeX $  Diagrams shortcode supported by mermaid  Interactive data visualization shortcode supported by ECharts  Mapbox shortcode supported by Mapbox GL JS  Music player shortcode supported by APlayer and MetingJS  Bilibili player shortcode  Kinds of admonitions shortcode  Custom style shortcode  Custom script shortcode  Animated typing supported by TypeIt  Cookie consent banner supported by cookieconsent … ","date":"2019-08-02","objectID":"/about/:0:1","series":null,"tags":null,"title":"About DoIt","uri":"/about/#features"},{"categories":null,"content":"Features Performance and SEO  Optimized for performance: 99/100 on mobile and 100/100 on desktop in Google PageSpeed Insights  PJAX loading supported  Optimized SEO performance with a correct SEO SCHEMA based on JSON-LD  Google Analytics supported  Fathom Analytics supported  Baidu Analytics supported  Umami Analytics supported  Plausible Analytics supported  Search engine verification supported (Google, Bind, Yandex and Baidu)  CDN for third-party libraries supported  Automatically converted images with Lazy Load by lazysizes Appearance and Layout / Responsive layout / Light/Dark mode  Globally consistent design language  Pagination supported  Easy-to-use and self-expanding table of contents  Multilanguage supported and i18n ready  Beautiful CSS animation Social and Comment Systems  Gravatar supported by Gravatar  Local Avatar supported  Up to 64 social links supported  Up to 28 share sites supported  Disqus comment system supported by Disqus  Gitalk comment system supported by Gitalk  Valine comment system supported by Valine  Waline comment system supported by Waline  Facebook comments system supported by Facebook  Telegram comments system supported by Comments  Commento comment system supported by Commento  Utterances comment system supported by Utterances  Twikoo comment system supported by Twikoo  Vssue comment system supported by Vssue  Remark42 comment system supported by Remark42  giscus comment system supported by giscus Extended Features  Search supported by Lunr.js or algolia or Fuse.js  Twemoji supported  Automatically highlighting code  Copy code to clipboard with one click  Images gallery supported by lightgallery.js  Extended Markdown syntax for Font Awesome icons  Extended Markdown syntax for ruby annotation  Extended Markdown syntax for fraction  Mathematical formula supported by $ \\KaTeX $  Diagrams shortcode supported by mermaid  Interactive data visualization shortcode supported by ECharts  Mapbox shortcode supported by Mapbox GL JS  Music player shortcode supported by APlayer and MetingJS  Bilibili player shortcode  Kinds of admonitions shortcode  Custom style shortcode  Custom script shortcode  Animated typing supported by TypeIt  Cookie consent banner supported by cookieconsent … ","date":"2019-08-02","objectID":"/about/:0:1","series":null,"tags":null,"title":"About DoIt","uri":"/about/#performance-and-seo"},{"categories":null,"content":"Features Performance and SEO  Optimized for performance: 99/100 on mobile and 100/100 on desktop in Google PageSpeed Insights  PJAX loading supported  Optimized SEO performance with a correct SEO SCHEMA based on JSON-LD  Google Analytics supported  Fathom Analytics supported  Baidu Analytics supported  Umami Analytics supported  Plausible Analytics supported  Search engine verification supported (Google, Bind, Yandex and Baidu)  CDN for third-party libraries supported  Automatically converted images with Lazy Load by lazysizes Appearance and Layout / Responsive layout / Light/Dark mode  Globally consistent design language  Pagination supported  Easy-to-use and self-expanding table of contents  Multilanguage supported and i18n ready  Beautiful CSS animation Social and Comment Systems  Gravatar supported by Gravatar  Local Avatar supported  Up to 64 social links supported  Up to 28 share sites supported  Disqus comment system supported by Disqus  Gitalk comment system supported by Gitalk  Valine comment system supported by Valine  Waline comment system supported by Waline  Facebook comments system supported by Facebook  Telegram comments system supported by Comments  Commento comment system supported by Commento  Utterances comment system supported by Utterances  Twikoo comment system supported by Twikoo  Vssue comment system supported by Vssue  Remark42 comment system supported by Remark42  giscus comment system supported by giscus Extended Features  Search supported by Lunr.js or algolia or Fuse.js  Twemoji supported  Automatically highlighting code  Copy code to clipboard with one click  Images gallery supported by lightgallery.js  Extended Markdown syntax for Font Awesome icons  Extended Markdown syntax for ruby annotation  Extended Markdown syntax for fraction  Mathematical formula supported by $ \\KaTeX $  Diagrams shortcode supported by mermaid  Interactive data visualization shortcode supported by ECharts  Mapbox shortcode supported by Mapbox GL JS  Music player shortcode supported by APlayer and MetingJS  Bilibili player shortcode  Kinds of admonitions shortcode  Custom style shortcode  Custom script shortcode  Animated typing supported by TypeIt  Cookie consent banner supported by cookieconsent … ","date":"2019-08-02","objectID":"/about/:0:1","series":null,"tags":null,"title":"About DoIt","uri":"/about/#appearance-and-layout"},{"categories":null,"content":"Features Performance and SEO  Optimized for performance: 99/100 on mobile and 100/100 on desktop in Google PageSpeed Insights  PJAX loading supported  Optimized SEO performance with a correct SEO SCHEMA based on JSON-LD  Google Analytics supported  Fathom Analytics supported  Baidu Analytics supported  Umami Analytics supported  Plausible Analytics supported  Search engine verification supported (Google, Bind, Yandex and Baidu)  CDN for third-party libraries supported  Automatically converted images with Lazy Load by lazysizes Appearance and Layout / Responsive layout / Light/Dark mode  Globally consistent design language  Pagination supported  Easy-to-use and self-expanding table of contents  Multilanguage supported and i18n ready  Beautiful CSS animation Social and Comment Systems  Gravatar supported by Gravatar  Local Avatar supported  Up to 64 social links supported  Up to 28 share sites supported  Disqus comment system supported by Disqus  Gitalk comment system supported by Gitalk  Valine comment system supported by Valine  Waline comment system supported by Waline  Facebook comments system supported by Facebook  Telegram comments system supported by Comments  Commento comment system supported by Commento  Utterances comment system supported by Utterances  Twikoo comment system supported by Twikoo  Vssue comment system supported by Vssue  Remark42 comment system supported by Remark42  giscus comment system supported by giscus Extended Features  Search supported by Lunr.js or algolia or Fuse.js  Twemoji supported  Automatically highlighting code  Copy code to clipboard with one click  Images gallery supported by lightgallery.js  Extended Markdown syntax for Font Awesome icons  Extended Markdown syntax for ruby annotation  Extended Markdown syntax for fraction  Mathematical formula supported by $ \\KaTeX $  Diagrams shortcode supported by mermaid  Interactive data visualization shortcode supported by ECharts  Mapbox shortcode supported by Mapbox GL JS  Music player shortcode supported by APlayer and MetingJS  Bilibili player shortcode  Kinds of admonitions shortcode  Custom style shortcode  Custom script shortcode  Animated typing supported by TypeIt  Cookie consent banner supported by cookieconsent … ","date":"2019-08-02","objectID":"/about/:0:1","series":null,"tags":null,"title":"About DoIt","uri":"/about/#social-and-comment-systems"},{"categories":null,"content":"Features Performance and SEO  Optimized for performance: 99/100 on mobile and 100/100 on desktop in Google PageSpeed Insights  PJAX loading supported  Optimized SEO performance with a correct SEO SCHEMA based on JSON-LD  Google Analytics supported  Fathom Analytics supported  Baidu Analytics supported  Umami Analytics supported  Plausible Analytics supported  Search engine verification supported (Google, Bind, Yandex and Baidu)  CDN for third-party libraries supported  Automatically converted images with Lazy Load by lazysizes Appearance and Layout / Responsive layout / Light/Dark mode  Globally consistent design language  Pagination supported  Easy-to-use and self-expanding table of contents  Multilanguage supported and i18n ready  Beautiful CSS animation Social and Comment Systems  Gravatar supported by Gravatar  Local Avatar supported  Up to 64 social links supported  Up to 28 share sites supported  Disqus comment system supported by Disqus  Gitalk comment system supported by Gitalk  Valine comment system supported by Valine  Waline comment system supported by Waline  Facebook comments system supported by Facebook  Telegram comments system supported by Comments  Commento comment system supported by Commento  Utterances comment system supported by Utterances  Twikoo comment system supported by Twikoo  Vssue comment system supported by Vssue  Remark42 comment system supported by Remark42  giscus comment system supported by giscus Extended Features  Search supported by Lunr.js or algolia or Fuse.js  Twemoji supported  Automatically highlighting code  Copy code to clipboard with one click  Images gallery supported by lightgallery.js  Extended Markdown syntax for Font Awesome icons  Extended Markdown syntax for ruby annotation  Extended Markdown syntax for fraction  Mathematical formula supported by $ \\KaTeX $  Diagrams shortcode supported by mermaid  Interactive data visualization shortcode supported by ECharts  Mapbox shortcode supported by Mapbox GL JS  Music player shortcode supported by APlayer and MetingJS  Bilibili player shortcode  Kinds of admonitions shortcode  Custom style shortcode  Custom script shortcode  Animated typing supported by TypeIt  Cookie consent banner supported by cookieconsent … ","date":"2019-08-02","objectID":"/about/:0:1","series":null,"tags":null,"title":"About DoIt","uri":"/about/#extended-features"},{"categories":null,"content":"License DoIt is licensed under the MIT license. Check the LICENSE file for details. Thanks to the authors of following resources included in the theme: normalize.css Font Awesome Simple Icons Animate.css autocomplete.js Lunr.js algoliasearch Fuse.js lazysizes object-fit-images Twemoji lightgallery.js clipboard.js Sharer.js TypeIt $ \\KaTeX $ mermaid ECharts Mapbox GL JS APlayer MetingJS Gitalk Valine Waline Twikoo Vssue cookieconsent Pjax Topbar Remark42 ","date":"2019-08-02","objectID":"/about/:0:2","series":null,"tags":null,"title":"About DoIt","uri":"/about/#license"},{"categories":null,"content":" You are not connected to the Internet, only cached pages will be available. ","date":"0001-01-01","objectID":"/offline/:0:0","series":null,"tags":null,"title":"Offline","uri":"/offline/#"}]